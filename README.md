
###  _DeepQ Decoding_

This repository provides all the tools necessary to obtain and run surface code decoders for fault tolerant quantum computation, i.e. decoders capable of dealing with faulty syndrome measurements, via <a href="https://www.nature.com/articles/nature14236">DeepQ reinforcement learning</a>.

We begin with an introduction to the setting, strategy and techniques, before providing detailed working examples for:

<ul>
  <li>Training and evaulating new decoders from scratch.</li>
  <li>Loading and running pre-trained decoders.</li>
</ul> 

Finally, we provide some of the results obtained from the trained models that have been provided here.

Enjoy!

#### 1) Setting

Topological quantum error correcting codes, and in particular the surface code, currently provide the most promising path to <a href="https://www.nature.com/articles/nature14236"> scalable fault tolerant quantum computation</a>. While a variety of decoders exist for such codes, such as <a href="https://arxiv.org/abs/1709.02154"> minimum weight perfect matching</a>, recently decoders obtained via  <a href="https://arxiv.org/abs/1802.06441"> machine learning techniques</a> have attracted attention due to both their potential flexibility, with respect to codes and noise models, and their potentially fast run times. However, up until this point it has not been demonstrated how one could use machine learning techniques to obtain decoders for the setting of fault tolerant quantum computation, in which both the physical qubits and syndrome extraction is assumed to be noisy. Here, we demonstrate how reinforcement learning techniques, and in particular deepQ learning, can be utilized to solve this problem and obtain such decoders.

##### 1a) The Surface Code

While the techniques presented here could be applied to any stabilizer code (work-in-progress) we focus on the surface code, as shown below: 

<p align="center">
<img src="https://user-images.githubusercontent.com/6330346/45884807-21499200-bdb5-11e8-97f7-60c9682c299e.png" width="75%" height="75%">
</p>

<ul>
  <li> We consider a d by d lattice, with qubits on the vertices (referred to as physical qubits), and plaquette stabilizers (a).</li>
  <li> Orange (blue) plaquettes indicate stabilizers which check the Z (X) parity of qubits on the vertices of the plaquette (b).</li>
  <li> Using red circles to indicate violated stabilizers (anyons) we see here some basic examples of the anyons created from X, Y or Z Pauli flips on a given vertex qubit (c).</li>
   <li> The X logical operator of the surface code we consider is given by any continuous string of X errors which connect the top and bottom boundaries of the code (d).
   <li> Similarly, The Z logical operator is given by any continuous string of Z errors which connect the left and right boundaries of the code (e).
</ul> 

In order to get intuition for the decoding problem, which we will present in detail further down, it is useful to see some examples of the syndromes (anyon configurations) generated by various error configurations...

<p align="center">
<img src="https://user-images.githubusercontent.com/6330346/45884806-21499200-bdb5-11e8-9770-f7eaf37159fb.png" width="40%" height="40%">
</p>

In particular, it is very important to note that the map from syndromes to error configurations is _not_ one-to-one! For example, one can see that the error configurations given in the top-left and bottom-left codes both lead to the same syndrome. This ambiguity in the error configuration leading to a given syndrome gives rise to the decoding problem, which we describe below.

##### 1b) The Decoding Problem

Given the above introduction to the surface code it is now possible to understand the decoding problem, within the fault tolerant setting. Quite loosely, given a code in the eigenstate of one of the logical operators (or, more generally, given any state in the ground state space of the code), the key aim of decoding is keep the code in this given state by exploiting _faulty_ syndrome information to determine which corrections need to be applied to the code to compensate for continuous noise and errors.

<p align="center">
<img src="https://user-images.githubusercontent.com/6330346/45884801-20b0fb80-bdb5-11e8-957d-e84e99fee7d6.png" width="80%" height="80%">
</p>

To be more specific, lets consider the above illustration:

<ol>
  <li>In the top left, we start with a code in which each physical qubit is prepared in the |0\rangle state, as a result of which the logical qubit (given by the whole code) is in the eigenstate of the logical Z operator. Our goal is to maintain the logical qubit in this state. </li>
  <li>Now, while storing the logical qubit (between gates for instance) the physical qubits are subject to noise. We consider depolarizing noise here for simplicity, for which in each unit of time each physical qubit is subject to either a Pauli X, Y or Z flip with a given probability (the physical error rate). In the above illustration, we imagine an X flip occuring on the physical qubit in the third row and second column.  </li>
  <li>In order to maintain the code in the state it was given to us, we therefore need to perform a correction by applying an X gate to the qubit which was randomly flipped. To do this, we need perform a syndrome extraction, from which our decoding algorithm can attempt to diagnose the error configuration which gave rise to the received syndrome. However, as illustrated in the diagram, the syndrome extraction process is also noisy, and for each stabilizer there is a probability (the measurement error rate) that the measured stabilizer value is incorrect - i.e. that we see an anyone where there is not one, or no anyon where there actually is one.</li>
   <li> To deal with this situation, instead of providing a single syndrome to the decoder, we perform multiple (faulty) syndrome measurements, between which physical errors may also occur. We then provide as input to our decoder not a single syndrome, but a stacked volume of succesive syndrome slices.
   <li> From this syndrome volume the decoding algorithm needs to suggest corrections which when applied to the code lattice move the logical qubit back into the original state (in practice, these corrections are not actually implemented, but rather tracked through the computation, and applied in a single step at the end).
   <li> In the ideal case the decoder will be able to correctly diagnose a sufficient proportion of syndrome volumes, such that the probability of an error occuring on the logical qubit is lower than the physical error rate on a physical qubit.
</ol> 

##### 1c) DeepQ Learning as a Tool for Obtaining Decoders

Given the problem as specified above, we utilize <a href="https://www.nature.com/articles/nature14236">DeepQ</a> <a href="http://incompleteideas.net/book/bookdraft2017nov5.pdf">reinforcement learning</a>, a technique which has been sucessfully used to obtain agents capable of super-human performance in domains such as  <a href="https://www.nature.com/articles/nature14236">Atari</a>, to obtain decoders which are capable of dealing with faulty measurements up to a threshold physical and measurement error rate. We will not go too deeply into the details and theory of Q-learning here, as an excellent introduction can be found in the fantastic textbook of <a href="http://incompleteideas.net/book/bookdraft2017nov5.pdf">Sutton and Barto</a>, which is strongly recommended.

However, to give a brief overview, the rough idea is that we will utilize a deep neural network (a convolutional neural network in our case) to parameterize the Q-function of a decoding agent, which interacts with the code lattice (the environment). This Q-function is a function which maps from states of the environment - syndrome volumes plus histories of previously applied corrections - to a Q-value for each available correction, where the Q-value of a given action, with respect to a particular environment state, encodes the expected long term benefit (not the exact technical definition!) to the agent of applying that correction when in that state. Given the Q-values corresponding to a given environment state, the optimal correction strategy then corresponds to applying the correction with the largest Q-value. Within this framework, the goal is then to obtain the optimal Q-function, which is done by letting the agent interact with the environment, during which the agents experiences are used to iteratively update the Q-function.

In order to present our approach it is therefore necessary to discuss:

<ul>
  <li> The manner in which we encode the environment state.</li>
  <li> The parameterization of our Q-function via a deep neural network.</li>
  <li> The procedure via which the agent interacts with the environment to gain experience, from which the Q-function can be updated.</li>
</ul> 

Let's begin with the manner in which the environment state is encoded. In particular, at any given time we consider the environment state to consist of:

<ol>
  <li> A representation of the most recently measured faulty syndrome volume.</li>
  <li> A representation of the actions which have been taken since receiving the most recent syndrome volume.</li>
</ol> 

Given a d by d surface code lattice, we encode a single syndrome slice in a (2d+1) by (2d + 1) binary matrix, as illustrated below:

<p align="center">
<img src="https://user-images.githubusercontent.com/6330346/45884802-20b0fb80-bdb5-11e8-906e-b758177a7c63.png" width="60%" height="60%">
</p>

Similarly, we encode the history of either X or Z Pauli corrections applied since the last syndrome volume was received in a (2d+1) by (2d + 1) binary matrix of the following form:

<p align="center">
<img src="https://user-images.githubusercontent.com/6330346/45884804-21499200-bdb5-11e8-86f6-f56a46c59567.png" width="60%" height="60%">
</p>

Finally, given these conventions for syndrome and action history slices we can construct the complete environment state by stacking syndrome slices on top of an action history slice for each allowed Pauli operator (in pratice we only need to allow for X and Z corrections). This gives us a total environment state in this form:

<p align="center">
<img src="https://user-images.githubusercontent.com/6330346/45884799-20b0fb80-bdb5-11e8-9313-0ffa63ae7382.png" width="80%" height="80%">
</p>

In the above image we have shown just three syndrome slices for simplicity, but as we will see later the depth of the syndrome volume (the number of slices) can be chosen at will.

Now that we know how the state of the environment is encoded at any given time step we can proceed to examine the way in which we choose to parameterize the Q-function of our agent via a deep convolutional neural network. For an introduction to such networks, see <a href="http://colah.github.io/posts/2014-07-Conv-Nets-Modular/">here</a> or <a href="https://github.com/R-Sweke/CrashCourseInNeuralNetworksWithKeras">here</a>.

<p align="center">
<img src="https://user-images.githubusercontent.com/6330346/45884803-20b0fb80-bdb5-11e8-8d20-c6c2fd337649.png" width="80%" height="80%">
</p>

As illustrated above, our deepQ network is given by a simple convolutional neural network, consisting of:

<ol>
  <li> A user-specified number of convolutional layers (a-b).</li>
  <li> A user specified number of feed-forward layers (c).</li>
  <li> A final layer providing Q-values for each available correction (d), with respect to the input state.</li>
</ol>  

Given these ingredients we can now examine in detail the training procedure, through which an optimal Q-function is updated via iterative updates from experience generated by interaction with the environment. As per the majority of reinforcement learning techniques, and illustrated below, this procedure involves a sequence of alternating steps in which:

<p align="center">
<img src="https://user-images.githubusercontent.com/6330346/45888570-a6857480-bdbe-11e8-92b0-d9730a95a455.png" width="40%" height="40%">

<ol>
  <li> The environment provides a state to the agent.</li>
  <li> The agent uses its current strategy to choose an action, with which it acts on the environment.</li>
  <li> The environment updates its internal state appropriately, and responds to the agent by providing a new state along with a numerical reward and a binary signal which illustrates whether the agent is "dead" or "alive".</li>
    <li> If the agent hasn't "died", it can then use this reward signal to update its internal strategy before once again acting on the environment and starting another round of interaction. If it has died, a new episode is started. </li>  
</ol> 

From the agent's perspective the goal is to converge to a strategy which allows it to maximise the expected value of its (discounted) cumulative reward. In our particular context of the decoding problem, an episode works as illustrated below:

<p align="center">
<img src="https://user-images.githubusercontent.com/6330346/45884798-20b0fb80-bdb5-11e8-9ef3-23b55b52a498.png" width="80%" height="80%">
</p>

In particular:

<ol>
  <li> As illustrated and described in Section 1b. (the "Decoding Problem"), an episode starts with the extraction of a (faulty) syndrome volume from the code (a, b). If the syndrome volume is trivial, i.e. there is not a single anyon in the entire volume, then another syndrome volume is extracted.</li>
  <li> As a new syndrome volume has just been extracted, the action history is reset to all zeros (c).</li>
  <li> The just extracted syndrome volume is combined with the reset action history, as previously described in the "state construction" figure, and then provided to the agent as the initial state (d).</li>
    <li> Now the agent must choose an action (e). As per most RL algorithms it is helpful to balance a period of exploration, with a period of exploiting previously obtained knowledge. As such, with a given probability \epsilon, which is annealed during the course of training, the agent will choose an action at random, and with a probability 1-\epsilon the agent will choose the action with the maximal Q-value according to its current parameterization. In order to aid training, we restrict the agents random choice to actions which are either adjacent to anyons, or adjacent to previously acted on qubits.</li>
    <li> When the agents acts on the environment with the chosen action, provided the action is not the identity action, multiple things then happen simultaneously. Firstly, the action history slices of the visible state are updated to indicate the action that has been applied (f). Then, the action is actually applied to the code lattice, whose error configuration is updated accordingly (g). Then finally, in order to determine the reward, a "referee" decoder takes in the true non-faulty syndrome corresponding to the updated error configuration (h). If the referee decoder can succesfully decode the current syndrome, then the agent remains alive and the episode continues, if not then the agent dies and the episode ends. If the agent remains alive and its action has resulted in putting the code back into the desired initial state, the agent is giving a reward of 1, in any other case the agent is given a reward of 0.</li>
    <li> The reward and game over signal is then combined with the updated state (in which only the action history was updated) and provided to the agent (i,j). In addition, the tuple of (state, action, reward, new state, game_over) is added to an external memory which is used to update the parameterization of the agent via backpropogation. </li>
        <li> The procedure detailed above is then repeated (k-p) until the point at which the agent chooses to do the identity (q), which can be done explicitly, or by repeating an action. Conceptually, the identity action is meant as a tool for the agent to signal its belief that it has applied all the corrections necessary to return the code to the desired initial state. </li>    
    <li> Given the identity signal from the agent, the environment then provides a new faulty syndrome volume (s,t), the action history slices of the state are reset, the new visible state is constructed from the rest action history and the updated syndrome (u,t) and fed to the agent, from which the episode continues as per steps (4-7), until the agent dies. </li> 
</ol> 

What has not been specifically illustrated in the above diagram is the procedure via which the parameterization of the Q-function is updated from batches of experience tuples. We will not present the details here as this is done using the exact same Q learning methodoloy described in  <a href="https://www.nature.com/articles/nature14236">these</a>  <a href="https://arxiv.org/abs/1511.06581">two</a> landmark papers on deepQ learning.

At this point all that remains is to discuss how decoding is done in practice once training has been completed and the agent has converged to an optimal Q-function. As illustrated below, this is quite straightforward:

<p align="center">
<img src="https://user-images.githubusercontent.com/6330346/45884796-20186500-bdb5-11e8-96e1-36ef03160bcf.png" width="80%" height="80%">
</p>

Specifically, decoding proceeds as follows:

<ol>
  <li> Firstly, a syndrome volume is extracted from the code lattice and encododed as previously discussed (a,b). This encoded syndrome volume is then stacked with a blank action history to create the initial input state to the decoder (c, d).</li>
  <li> Given this input state, one forward pass of the neural network is executed and an argmax is taken over the output Q-values to obtain the first suggested correction. This suggested correction is then added to a memory (f) and used to update the action history slices of the visible state (e). These updated action history slices are then combined with the original syndrome volume (g) and passed to the decoder (h)</li>
  <li> Step 2 is then repeated (i,j,k,l) until the point at which the agent chooses the identity action (m).</li>
    <li> At this point, given that the agent has signalled that it belives it has supplied all the necessary corrections, the acummulated corrections are applied to the code lattice (n), or in practice, tracked through the computation. </li>
</ol> 




#### 2) Training Decoders in Practice

Now that we have discussed the conceptual foundations, strategies and techniques involved, we will provide detailed examples of how train decoders via the procedures discussed. In particular, we will first walk through a very simple script for training a decoder with a given set of hyper-parameters, before proceeding to discuss how to obtain optimal decoders for a range of error rates through an iterative training procedure involving a hyper-parameter optimization for each error rate.

If you would like to run the code discussed in this section, you can find the simple single point training script within the "Training Example" notebook in the example_notebooks folder of the repo, while the cluster scripts for the iterative training procedure can be found in the cluster_scripts folder of the repo.

##### 2a) Requirements

The following packages are required, and can be installed via PIP:

<ol>
  <li> Python 3 (with numpy and scipy)</li>
  <li> tensorflow </li>
  <li> keras </li> 
  <li> gym </li> 
</ol> 

In addition, a modified version of the Keras-RL package is required, which should be installed from <a href="https://github.com/R-Sweke/keras-rl">this fork</a>

##### 2b) A Simple Training Script

We begin by importing all required packages and methods:


```python
import numpy as np
import keras
import tensorflow
import gym

from Function_Library import *
from Environments import *

import rl as rl
from rl.agents.dqn import DQNAgent
from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy, LinearAnnealedPolicy, GreedyQPolicy
from rl.memory import SequentialMemory
from rl.callbacks import FileLogger

import json
import copy
import sys
import os
import shutil
import datetime
```

We then proceed by providing all required hyperparameters and physical configuration settings. In order to allow for easier grid searching and incremented training later on we choose to split all hyperparameters into two categories:

   - fixed configs: These remain constant during the course of a grid search or incremented training procedure.
   - variable configs: We will later set up training grids over these hyperparameters.
    
In particular, the fixed parameters one must provide are:

   1. **d**: The lattice width (equal to the lattice height)
   2. **use_Y**: If true then the agent can perform Y Pauli flips directly, if False then the agent can only perform X and Z Pauli flips.
   3. **train_freq**: The number of agent-environment interaction steps which occur between each updating of the agent's weights.
   4. **batch_size**: The size of batches used for calculating loss functions for gradient descent updates of agent weights.
   5. **print_freq**: Every print_freq episodes the statistics of the training procedure will be logged.
   6. **rolling_average_length**: The number of most recent episodes over which any relevant rolling average will be calculated.
   7. **stopping_patience**: The number of episodes after which no improvement will result in the early stopping of the training procedure.
   8. **error_model**: A string in ["X", "DP"], specifiying the noise model of the environment as X flips only or depolarizing noise.
   9. **c_layers**: A list of lists specifying the structure of the convolutional layers of the agent deepQ network. Each inner list describes a layer and has the form [num_filters, filter_width, stride].
   10. **ff_layers**: A list of lists specifying the structure of the feed-forward neural network sitting on top of the convolutional neural network. Each inner list has the form [num_neurons, output_dropout_rate].
   11. **max_timesteps**: The maximum number of training timesteps allowed.
   12. **volume_depth**: The number of syndrome measurements taken each time a new syndrome extraction is performed - i.e. the depth of the syndrome volume passed to the agent.
   13. **testing_length**: The number of episodes uses to evaluate the trained agents performance. 
   14. **buffer_size**: The maximum number of experience tuples held in the memory from which the update batches for agent updating are drawn.
   15. **dueling**: A boolean indicating whether or not a [dueling architecture](https://arxiv.org/abs/1511.06581) should be used.
   16. **masked_greedy**: A boolean which indicates whether the agent will only be allowed to choose legal actions (actions next to an anyon or previously flipped qubit) when acting greedily (i.e. when choosing actions via the argmax of the Q-values)
   17. **static_decoder**: For training within the fault tolerant setting (multi-cycle decoding) this should always be set to True.
   
In addition, the parameters which we will later incrementally vary or grid search around are:

   1. **p_phys**: The physical error probability
   2. **p_meas**: The measurement error probability
   3. **success_threshold**: The qubit lifetime rolling average at which training has been deemed succesfull and will be stopped.
   4. **learning_starts**: The number of initial steps taken to contribute experience tuples to memory before any weight updates are made.
   5. **learning_rate**: The learning rate for gradient descent optimization (via the Adam optimizer)
   6. **exploration_fraction**: The number of time steps over which epsilon, the parameter controlling the probability of a random explorative action, is annealed.
   7. **max_eps**: The initial maximum value of epsilon.
   8. **target_network_update_freq**: In order to achieve stable training, a target network is cloned off from the active deepQ agent every target_network_update_freq interval of steps. This target network is then used to generate the target Q-function over the following interval.
   9. **gamma**: The discount rate used for calculating the expected discounted cumulative return (the Q-values).
   10. **final_eps**: The final value at which annealing of epsilon will be stopped.
   
Furthermore, in addition to all the above parameters one must provide a directory into which results and training progress as logged, as well as the path to a pre-trained referee decoder. Here e provide two pre-trained feed forward classification based referee decoders, one for X noise and one for DP noise. However, in principle any perfect-measurement decoding algorithm (such as MWPM) could be used here.


```python
fixed_configs = {"d": 5,
                "use_Y": False,
                "train_freq": 1,
                "batch_size": 32,
                "print_freq": 250,
                "rolling_average_length": 500,
                "stopping_patience": 500,
                "error_model": "X",
                "c_layers": [[64,3,2],[32,2,1],[32,2,1]],
                "ff_layers": [[512,0.2]],
                "max_timesteps": 1000000,
                "volume_depth": 5,
                "testing_length": 101,
                "buffer_size": 50000,
                "dueling": True,
                "masked_greedy": False,
                "static_decoder": True}

variable_configs = {"p_phys": 0.001,
                    "p_meas": 0.001,
                    "success_threshold": 10000,
                    "learning_starts": 1000,
                    "learning_rate": 0.00001,
                    "exploration_fraction": 100000,
                    "max_eps": 1.0,
                    "target_network_update_freq": 5000,
                    "gamma": 0.99,
                    "final_eps": 0.02}

logging_directory = os.path.join(os.getcwd(),"logging_directory/")
static_decoder_path = os.path.join(os.getcwd(),"referee_decoders/nn_d5_X_p5")


all_configs = {}

for key in fixed_configs.keys():
    all_configs[key] = fixed_configs[key]

for key in variable_configs.keys():
    all_configs[key] = variable_configs[key]

static_decoder = load_model(static_decoder_path)                                                 
logging_path = os.path.join(logging_directory,"training_history.json")
logging_callback = FileLogger(filepath = logging_path,interval = all_configs["print_freq"])
```

Now that we have specified all the required parameters we can instantiate our environment:


```python
env = Surface_Code_Environment_Multi_Decoding_Cycles(d=all_configs["d"], 
    p_phys=all_configs["p_phys"], 
    p_meas=all_configs["p_meas"],  
    error_model=all_configs["error_model"], 
    use_Y=all_configs["use_Y"], 
    volume_depth=all_configs["volume_depth"],
    static_decoder=static_decoder)
```

The environment class is defined to mirror the environments of [https://gym.openai.com/](openAI gym), and such contains the required "reset" and "step" methods, via which the agent can interact with the environment, in addition to decoding specific methods and attributes whose details can be found in the relevant method docstrings.

We can now proceed to define the agent. We being by specifying the memory to be used, as well as the exploration and testing policies.


```python
memory = SequentialMemory(limit=all_configs["buffer_size"], window_length=1)

policy = LinearAnnealedPolicy(EpsGreedyQPolicy(masked_greedy=all_configs["masked_greedy"]), 
    attr='eps', value_max=all_configs["max_eps"], 
    value_min=all_configs["final_eps"], 
    value_test=0.0, 
    nb_steps=all_configs["exploration_fraction"])

test_policy = GreedyQPolicy(masked_greedy=True)
```

Finally, we can then build the deep convolutional neural network which will represent our Q-function and compile our agent.


```python
model = build_convolutional_nn(all_configs["c_layers"], 
                               all_configs["ff_layers"], 
                               env.observation_space.shape, 
                               env.num_actions)

dqn = DQNAgent(model=model, 
               nb_actions=env.num_actions, 
               memory=memory, 
               nb_steps_warmup=all_configs["learning_starts"], 
               target_model_update=all_configs["target_network_update_freq"], 
               policy=policy,
               test_policy = test_policy,
               gamma = all_configs["gamma"],
               enable_dueling_network=all_configs["dueling"])  


dqn.compile(Adam(lr=all_configs["learning_rate"]))
```

With both the agent and the environment specified, it is then possible to train the agent by calling the agent's "fit" method. If you want to run this on a single computer, be careful, it may take up to 12 hours!


```python
now = datetime.datetime.now()
started_file = os.path.join(logging_directory,"started_at.p")
pickle.dump(now, open(started_file, "wb" ) )

history = dqn.fit(env, 
  nb_steps=all_configs["max_timesteps"], 
  action_repetition=1, 
  callbacks=[logging_callback], 
  verbose=2,
  visualize=False, 
  nb_max_start_steps=0, 
  start_step_policy=None, 
  log_interval=all_configs["print_freq"],
  nb_max_episode_steps=None, 
  episode_averaging_length=all_configs["rolling_average_length"], 
  success_threshold=all_configs["success_threshold"],
  stopping_patience=all_configs["stopping_patience"],
  min_nb_steps=all_configs["exploration_fraction"],
  single_cycle=False)
```

During the training procedure various statistics are logged, at the specified episode frequency, to both stdout and to the file "training_history.json" in the specified directory:

    Training for 1000000 steps ...
    -----------------
                    
    Episode: 250
    Step: 2232/1000000
    This Episode Steps: 4
    This Episode Reward: 0.0
    This Episode Duration: 0.122s
    Rolling Lifetime length: 38.000
    Best Lifetime Rolling Avg: 52.857142857142854
    Best Episode: 6
    Time Since Best: 243
    Has Succeeded: False
    Stopped Improving: False
    Metrics: loss: 0.024631, mean_q: 0.104172, mean_eps: 0.978151
    Total Training Time: 42.201s
    
    -----------------
                    
    Episode: 500
    Step: 4482/1000000
    This Episode Steps: 7
    This Episode Reward: 1.0
    This Episode Duration: 0.201s
    Rolling Lifetime length: 39.290
    Best Lifetime Rolling Avg: 52.857142857142854
    Best Episode: 6
    Time Since Best: 493
    Has Succeeded: False
    Stopped Improving: False
    Metrics: loss: 0.023562, mean_q: 0.120933, mean_eps: 0.956116
    Total Training Time: 106.792s
    
And training goes on...
    
    -----------------
                    
    Episode: 6000
    Step: 89611/1000000
    This Episode Steps: 37
    This Episode Reward: 24.0
    This Episode Duration: 1.061s
    Rolling Lifetime length: 279.420
    Best Lifetime Rolling Avg: 279.42
    Best Episode: 5999
    Time Since Best: 0
    Has Succeeded: False
    Stopped Improving: False
    Metrics: loss: 0.343747, mean_q: 4.159633, mean_eps: 0.121998
    Total Training Time: 2478.817s
    
    -----------------
                    
    Episode: 6250
    Step: 126262/1000000
    This Episode Steps: 784
    This Episode Reward: 566.0
    This Episode Duration: 21.441s
    Rolling Lifetime length: 1020.830
    Best Lifetime Rolling Avg: 1020.83
    Best Episode: 6249
    Time Since Best: 0
    Has Succeeded: False
    Stopped Improving: False
    Metrics: loss: 0.580168, mean_q: 9.287714, mean_eps: 0.020000
    Total Training Time: 3490.853s
    
    Training Finished in 5840.354 seconds
            
    Final Step: 210321
    Succeeded: False
    Stopped_Improving: False
    Final Episode Lifetimes Rolling Avg: 2882.750


As you can see above, we manually stopped training after approximately 6000 seconds while the agent was still improving, and before it has reached the specified success threshold.

In order to evaluate the agent later on, or apply the agent in a production decoding scenario we can easily save the weights:


```python
weights_file = os.path.join(logging_directory, "dqn_weights.h5f")
dqn.save_weights(weights_file, overwrite=True)
```

And finally, in order to evaluate the training procedure we may be interested in viewing any of the metrics which were logged. These are all saved within the history.history dictionary. For example, we are often most interested in analyzing the training procedure by looking at the rolling average of the qubit lifetime, which we can do as follows:


```python
from matplotlib import pyplot as plt
%matplotlib inline

training_history = history.history["episode_lifetimes_rolling_avg"]

plt.figure(figsize=(12,7))
plt.plot(training_history)
plt.xlabel('Episode')
plt.ylabel('Rolling Average Qubit Lifetime')
_ = plt.title("Training History")
```

<p align="center">
<img src="https://user-images.githubusercontent.com/6330346/45919538-2121c300-be97-11e8-902e-a217734f33ed.png" width="60%" height="60%">
</p>

From the above plot one can see that during the exploration phase the agent was unable to do well, due to constant exploratory random actions, but was able to exploit this knowledge effectively once the exploration probability became sufficiently low. Again, it is also clear that the agent was definitely still learning and improving when we chose to stop the training procedure.


