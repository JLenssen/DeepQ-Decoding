
###  _DeepQ Decoding_

This repository provides all the tools necessary to obtain and run surface code decoders for fault tolerant quantum computation, i.e. decoders capable of dealing with faulty syndrome measurements, via <a href="https://www.nature.com/articles/nature14236">DeepQ reinforcement learning</a>.

We begin with an introduction to the setting, strategy and techniques, before providing detailed working examples for:

<ul>
  <li>Training and evaulating new decoders from scratch.</li>
  <li>Loading and running pre-trained decoders.</li>
</ul> 

Finally, we provide some of the results obtained from the trained models that have been provided here.

Enjoy!

#### 1) Setting

Topological quantum error correcting codes, and in particular the surface code, currently provide the most promising path to <a href="https://www.nature.com/articles/nature14236"> scalable fault tolerant quantum computation</a>. While a variety of decoders exist for such codes, such as <a href="https://arxiv.org/abs/1709.02154"> minimum weight perfect matching</a>, recently decoders obtained via  <a href="https://arxiv.org/abs/1802.06441"> machine learning techniques</a> have attracted attention due to both their potential flexibility, with respect to codes and noise models, and their potentially fast run times. However, up until this point it has not been demonstrated how one could use machine learning techniques to obtain decoders for the setting of fault tolerant quantum computation, in which both the physical qubits and syndrome extraction is assumed to be noisy. Here, we demonstrate how reinforcement learning techniques, and in particular deepQ learning, can be utilized to solve this problem and obtain such decoders.

##### 1a) The Surface Code

While the techniques presented here could be applied to any stabilizer code (work-in-progress) we focus on the surface code, as shown below: 

<p align="center">
<img src="https://user-images.githubusercontent.com/6330346/45884807-21499200-bdb5-11e8-97f7-60c9682c299e.png" width="75%" height="75%">
</p>

<ul>
  <li> We consider a d by d lattice, with qubits on the vertices (referred to as physical qubits), and plaquette stabilizers (a).</li>
  <li> Orange (blue) plaquettes indicate stabilizers which check the Z (X) parity of qubits on the vertices of the plaquette (b).</li>
  <li> Using red circles to indicate violated stabilizers (anyons) we see here some basic examples of the anyons created from X, Y or Z Pauli flips on a given vertex qubit (c).</li>
   <li> The X logical operator of the surface code we consider is given by any continuous string of X errors which connect the top and bottom boundaries of the code (d).
   <li> Similarly, The Z logical operator is given by any continuous string of Z errors which connect the left and right boundaries of the code (e).
</ul> 

In order to get intuition for the decoding problem, which we will present in detail further down, it is useful to see some examples of the syndromes (anyon configurations) generated by various error configurations...

<p align="center">
<img src="https://user-images.githubusercontent.com/6330346/45884806-21499200-bdb5-11e8-9770-f7eaf37159fb.png" width="40%" height="40%">
</p>

In particular, it is very important to note that the map from syndromes to error configurations is _not_ one-to-one! For example, one can see that the error configurations given in the top-left and bottom-left codes both lead to the same syndrome. This ambiguity in the error configuration leading to a given syndrome gives rise to the decoding problem, which we describe below.

##### 1b) The Decoding Problem

Given the above introduction to the surface code it is now possible to understand the decoding problem, within the fault tolerant setting. Quite loosely, given a code in the eigenstate of one of the logical operators (or, more generally, given any state in the ground state space of the code), the key aim of decoding is keep the code in this given state by exploiting _faulty_ syndrome information to determine which corrections need to be applied to the code to compensate for continuous noise and errors.

<p align="center">
<img src="https://user-images.githubusercontent.com/6330346/45884801-20b0fb80-bdb5-11e8-957d-e84e99fee7d6.png" width="80%" height="80%">
</p>

To be more specific, lets consider the above illustration:

<ol>
  <li>In the top left, we start with a code in which each physical qubit is prepared in the |0\rangle state, as a result of which the logical qubit (given by the whole code) is in the eigenstate of the logical Z operator. Our goal is to maintain the logical qubit in this state. </li>
  <li>Now, while storing the logical qubit (between gates for instance) the physical qubits are subject to noise. We consider depolarizing noise here for simplicity, for which in each unit of time each physical qubit is subject to either a Pauli X, Y or Z flip with a given probability (the physical error rate). In the above illustration, we imagine an X flip occuring on the physical qubit in the third row and second column.  </li>
  <li>In order to maintain the code in the state it was given to us, we therefore need to perform a correction by applying an X gate to the qubit which was randomly flipped. To do this, we need perform a syndrome extraction, from which our decoding algorithm can attempt to diagnose the error configuration which gave rise to the received syndrome. However, as illustrated in the diagram, the syndrome extraction process is also noisy, and for each stabilizer there is a probability (the measurement error rate) that the measured stabilizer value is incorrect - i.e. that we see an anyone where there is not one, or no anyon where there actually is one.</li>
   <li> To deal with this situation, instead of providing a single syndrome to the decoder, we perform multiple (faulty) syndrome measurements, between which physical errors may also occur. We then provide as input to our decoder not a single syndrome, but a stacked volume of succesive syndrome slices.
   <li> From this syndrome volume the decoding algorithm needs to suggest corrections which when applied to the code lattice move the logical qubit back into the original state (in practice, these corrections are not actually implemented, but rather tracked through the computation, and applied in a single step at the end).
   <li> In the ideal case the decoder will be able to correctly diagnose a sufficient proportion of syndrome volumes, such that the probability of an error occuring on the logical qubit is lower than the physical error rate on a physical qubit.
</ol> 

##### 1c) DeepQ Learning as a Tool for Obtaining Decoders

Given the problem as specified above, we utilize <a href="https://www.nature.com/articles/nature14236">DeepQ</a> <a href="http://incompleteideas.net/book/bookdraft2017nov5.pdf">reinforcement learning</a>, a technique which has been sucessfully used to obtain agents capable of super-human performance in domains such as  <a href="https://www.nature.com/articles/nature14236">Atari</a>, to obtain decoders which are capable of dealing with faulty measurements up to a threshold physical and measurement error rate. We will not go too deeply into the details and theory of Q-learning here, as an excellent introduction can be found in the fantastic textbook of <a href="http://incompleteideas.net/book/bookdraft2017nov5.pdf">Sutton and Barto</a>, which is strongly recommended.

However, to give a brief overview, the rough idea is that we will utilize a deep neural network (a convolutional neural network in our case) to parameterize the Q-function of a decoding agent, which interacts with the code lattice (the environment). This Q-function is a function which maps from states of the environment - syndrome volumes plus histories of previously applied corrections - to a Q-value for each available correction, where the Q-value of a given action, with respect to a particular environment state, encodes the expected long term benefit (not the exact technical definition!) to the agent of applying that correction when in that state. Given the Q-values corresponding to a given environment state, the optimal correction strategy then corresponds to applying the correction with the largest Q-value. Within this framework, the goal is then to obtain the optimal Q-function, which is done by letting the agent interact with the environment, during which the agents experiences are used to iteratively update the Q-function.

In order to present our approach it is therefore necessary to discuss:

<ul>
  <li> The manner in which we encode the environment state.</li>
  <li> The parameterization of our Q-function via a deep neural network.</li>
  <li> The procedure via which the agent interacts with the environment to gain experience, from which the Q-function can be updated.</li>
</ul> 

Let's begin with the manner in which the environment state is encoded. In particular, at any given time we consider the environment state to consist of:

<ol>
  <li> A representation of the most recently measured faulty syndrome volume.</li>
  <li> A representation of the actions which have been taken since receiving the most recent syndrome volume.</li>
</ol> 

Given a d by d surface code lattice, we encode a single syndrome slice in a (2d+1) by (2d + 1) binary matrix, as illustrated below:

<p align="center">
<img src="https://user-images.githubusercontent.com/6330346/45884802-20b0fb80-bdb5-11e8-906e-b758177a7c63.png" width="60%" height="60%">
</p>

Similarly, we encode the history of either X or Z Pauli corrections applied since the last syndrome volume was received in a (2d+1) by (2d + 1) binary matrix of the following form:

<p align="center">
<img src="https://user-images.githubusercontent.com/6330346/45884804-21499200-bdb5-11e8-86f6-f56a46c59567.png" width="60%" height="60%">
</p>

Finally, given these conventions for syndrome and action history slices we can construct the complete environment state by stacking syndrome slices on top of an action history slice for each allowed Pauli operator (in pratice we only need to allow for X and Z corrections). This gives us a total environment state in this form:

<p align="center">
<img src="https://user-images.githubusercontent.com/6330346/45884799-20b0fb80-bdb5-11e8-9313-0ffa63ae7382.png" width="80%" height="80%">
</p>

In the above image we have shown just three syndrome slices for simplicity, but as we will see later the depth of the syndrome volume (the number of slices) can be chosen at will.

Now that we know how the state of the environment is encoded at any given time step we can proceed to examine the way in which we choose to parameterize the Q-function of our agent via a deep convolutional neural network. For an introduction to such networks, see <a href="http://colah.github.io/posts/2014-07-Conv-Nets-Modular/">here</a> or <a href="https://github.com/R-Sweke/CrashCourseInNeuralNetworksWithKeras">here</a>.

<p align="center">
<img src="https://user-images.githubusercontent.com/6330346/45884803-20b0fb80-bdb5-11e8-8d20-c6c2fd337649.png" width="80%" height="80%">
</p>

As illustrated above, our deepQ network is given by a simple convolutional neural network, consisting of:

<ol>
  <li> A user-specified number of convolutional layers (a-b).</li>
  <li> A user specified number of feed-forward layers (c).</li>
  <li> A final layer providing Q-values for each available correction (d), with respect to the input state.</li>
</ol>  

Given these ingredients we can now examine in detail the training procedure, through which an optimal Q-function is updated via iterative updates from experience generated by interaction with the environment. As per the majority of reinforcement learning techniques, and illustrated below, this procedure involves a sequence of alternating steps in which:

<p align="center">
<img src="https://user-images.githubusercontent.com/6330346/45888570-a6857480-bdbe-11e8-92b0-d9730a95a455.png" width="40%" height="40%">

<ol>
  <li> The environment provides a state to the agent.</li>
  <li> The agent uses its current strategy to choose an action, with which it acts on the environment.</li>
  <li> The environment updates its internal state appropriately, and responds to the agent by providing a new state along with a numerical reward and a binary signal which illustrates whether the agent is "dead" or "alive".</li>
    <li> If the agent hasn't "died", it can then use this reward signal to update its internal strategy before once again acting on the environment and starting another round of interaction. If it has died, a new episode is started. </li>  
</ol> 

From the agent's perspective the goal is to converge to a strategy which allows it to maximise the expected value of its (discounted) cumulative reward. In our particular context of the decoding problem, an episode works as illustrated below:

<p align="center">
<img src="https://user-images.githubusercontent.com/6330346/45884798-20b0fb80-bdb5-11e8-9ef3-23b55b52a498.png" width="80%" height="80%">
</p>

In particular:

<ol>
  <li> As illustrated and described in Section 1b. (the "Decoding Problem"), an episode starts with the extraction of a (faulty) syndrome volume from the code (a, b). If the syndrome volume is trivial, i.e. there is not a single anyon in the entire volume, then another syndrome volume is extracted.</li>
  <li> As a new syndrome volume has just been extracted, the action history is reset to all zeros (c).</li>
  <li> The just extracted syndrome volume is combined with the reset action history, as previously described in the "state construction" figure, and then provided to the agent as the initial state (d).</li>
    <li> Now the agent must choose an action (e). As per most RL algorithms it is helpful to balance a period of exploration, with a period of exploiting previously obtained knowledge. As such, with a given probability \epsilon, which is annealed during the course of training, the agent will choose an action at random, and with a probability 1-\epsilon the agent will choose the action with the maximal Q-value according to its current parameterization. In order to aid training, we restrict the agents random choice to actions which are either adjacent to anyons, or adjacent to previously acted on qubits.</li>
    <li> When the agents acts on the environment with the chosen action, provided the action is not the identity action, multiple things then happen simultaneously. Firstly, the action history slices of the visible state are updated to indicate the action that has been applied (f). Then, the action is actually applied to the code lattice, whose error configuration is updated accordingly (g). Then finally, in order to determine the reward, a "referee" decoder takes in the true non-faulty syndrome corresponding to the updated error configuration (h). If the referee decoder can succesfully decode the current syndrome, then the agent remains alive and the episode continues, if not then the agent dies and the episode ends. If the agent remains alive and its action has resulted in putting the code back into the desired initial state, the agent is giving a reward of 1, in any other case the agent is given a reward of 0.</li>
    <li> The reward and game over signal is then combined with the updated state (in which only the action history was updated) and provided to the agent (i,j). In addition, the tuple of (state, action, reward, new state, game_over) is added to an external memory which is used to update the parameterization of the agent via backpropogation. </li>
        <li> The procedure detailed above is then repeated (k-p) until the point at which the agent chooses to do the identity (q), which can be done explicitly, or by repeating an action. Conceptually, the identity action is meant as a tool for the agent to signal its belief that it has applied all the corrections necessary to return the code to the desired initial state. </li>    
    <li> Given the identity signal from the agent, the environment then provides a new faulty syndrome volume (s,t), the action history slices of the state are reset, the new visible state is constructed from the rest action history and the updated syndrome (u,t) and fed to the agent, from which the episode continues as per steps (4-7), until the agent dies. </li> 
</ol> 

What has not been specifically illustrated in the above diagram is the procedure via which the parameterization of the Q-function is updated from batches of experience tuples. We will not present the details here as this is done using the exact same Q learning methodoloy described in  <a href="https://www.nature.com/articles/nature14236">these</a>  <a href="https://arxiv.org/abs/1511.06581">two</a> landmark papers on deepQ learning.

At this point all that remains is to discuss how decoding is done in practice once training has been completed and the agent has converged to an optimal Q-function. As illustrated below, this is quite straightforward:

<p align="center">
<img src="https://user-images.githubusercontent.com/6330346/45884796-20186500-bdb5-11e8-96e1-36ef03160bcf.png" width="80%" height="80%">
</p>

Specifically, decoding proceeds as follows:

<ol>
  <li> Firstly, a syndrome volume is extracted from the code lattice and encododed as previously discussed (a,b). This encoded syndrome volume is then stacked with a blank action history to create the initial input state to the decoder (c, d).</li>
  <li> Given this input state, one forward pass of the neural network is executed and an argmax is taken over the output Q-values to obtain the first suggested correction. This suggested correction is then added to a memory (f) and used to update the action history slices of the visible state (e). These updated action history slices are then combined with the original syndrome volume (g) and passed to the decoder (h)</li>
  <li> Step 2 is then repeated (i,j,k,l) until the point at which the agent chooses the identity action (m).</li>
    <li> At this point, given that the agent has signalled that it belives it has supplied all the necessary corrections, the acummulated corrections are applied to the code lattice (n), or in practice, tracked through the computation. </li>
</ol> 



### 2) Training Decoders in Practice

Now that we have discussed the conceptual foundations, strategies and techniques involved, lets walk through a detailed example of how to train a decoder.

#### 2a) Requirements

The following packages are required, and can be installed via PIP:

<ol>
  <li> Python 3 (with numpy and scipy)</li>
  <li> tensorflow </li>
  <li> keras </li> 
  <li> gym </li> 
</ol> 

In addition, a modified version of the Keras-RL package is required, which should be installed from <a href="https://github.com/R-Sweke/keras-rl">this fork</a>


```python
import numpy as np
import keras
import tensorflow
import gym

from Function_Library import *
from Environments import *

import rl as rl
from rl.agents.dqn import DQNAgent
from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy, LinearAnnealedPolicy, GreedyQPolicy
from rl.memory import SequentialMemory
from rl.callbacks import FileLogger

import json
import copy
import sys
import os
import shutil
import datetime
```

    Using TensorFlow backend.



```python
fixed_configs = {"d": 5,
                "use_Y": False,
                "train_freq": 1,
                "batch_size": 32,
                "print_freq": 250,
                "rolling_average_length": 500,
                "stopping_patience": 500,
                "error_model": "X",
                "c_layers": [[64,3,2],[32,2,1],[32,2,1]],
                "ff_layers": [[512,0.2]],
                "max_timesteps": 1000000,
                "volume_depth": 5,
                "testing_length": 101,
                "buffer_size": 50000,
                "dueling": True,
                "masked_greedy": False,
                "static_decoder": True}

variable_configs = {"p_phys": 0.001,
                    "p_meas": 0.001,
                    "success_threshold": 10000,
                    "learning_starts": 1000,
                    "learning_rate": 0.00001,
                    "exploration_fraction": 100000,
                    "max_eps": 1.0,
                    "target_network_update_freq": 5000,
                    "gamma": 0.99,
                    "final_eps": 0.02}

logging_directory = os.path.join(os.getcwd(),"logging_directory/")
static_decoder_path = os.path.join(os.getcwd(),"referee_decoders/nn_d5_X_p5")


all_configs = {}

for key in fixed_configs.keys():
    all_configs[key] = fixed_configs[key]

for key in variable_configs.keys():
    all_configs[key] = variable_configs[key]

static_decoder = load_model(static_decoder_path)                                                 
logging_path = os.path.join(logging_directory,"training_history.json")
logging_callback = FileLogger(filepath = logging_path,interval = all_configs["print_freq"])
```


```python
env = Surface_Code_Environment_Multi_Decoding_Cycles(d=all_configs["d"], 
    p_phys=all_configs["p_phys"], 
    p_meas=all_configs["p_meas"],  
    error_model=all_configs["error_model"], 
    use_Y=all_configs["use_Y"], 
    volume_depth=all_configs["volume_depth"],
    static_decoder=static_decoder)
```


```python
model = build_convolutional_nn(all_configs["c_layers"], 
                               all_configs["ff_layers"], 
                               env.observation_space.shape, 
                               env.num_actions)

memory = SequentialMemory(limit=all_configs["buffer_size"], window_length=1)

policy = LinearAnnealedPolicy(EpsGreedyQPolicy(masked_greedy=all_configs["masked_greedy"]), 
    attr='eps', value_max=all_configs["max_eps"], 
    value_min=all_configs["final_eps"], 
    value_test=0.0, 
    nb_steps=all_configs["exploration_fraction"])

test_policy = GreedyQPolicy(masked_greedy=True)
```


```python
dqn = DQNAgent(model=model, 
               nb_actions=env.num_actions, 
               memory=memory, 
               nb_steps_warmup=all_configs["learning_starts"], 
               target_model_update=all_configs["target_network_update_freq"], 
               policy=policy,
               test_policy = test_policy,
               gamma = all_configs["gamma"],
               enable_dueling_network=all_configs["dueling"])  


dqn.compile(Adam(lr=all_configs["learning_rate"]))
```


```python
now = datetime.datetime.now()
started_file = os.path.join(logging_directory,"started_at.p")
pickle.dump(now, open(started_file, "wb" ) )

history = dqn.fit(env, 
  nb_steps=all_configs["max_timesteps"], 
  action_repetition=1, 
  callbacks=[logging_callback], 
  verbose=2,
  visualize=False, 
  nb_max_start_steps=0, 
  start_step_policy=None, 
  log_interval=all_configs["print_freq"],
  nb_max_episode_steps=None, 
  episode_averaging_length=all_configs["rolling_average_length"], 
  success_threshold=all_configs["success_threshold"],
  stopping_patience=all_configs["stopping_patience"],
  min_nb_steps=all_configs["exploration_fraction"],
  single_cycle=False)
```

    Training for 1000000 steps ...
    -----------------
                    
    Episode: 250
    Step: 2232/1000000
    This Episode Steps: 4
    This Episode Reward: 0.0
    This Episode Duration: 0.122s
    Rolling Lifetime length: 38.000
    Best Lifetime Rolling Avg: 52.857142857142854
    Best Episode: 6
    Time Since Best: 243
    Has Succeeded: False
    Stopped Improving: False
    Metrics: loss: 0.024631, mean_q: 0.104172, mean_eps: 0.978151
    Total Training Time: 42.201s
    
    -----------------
                    
    Episode: 500
    Step: 4482/1000000
    This Episode Steps: 7
    This Episode Reward: 1.0
    This Episode Duration: 0.201s
    Rolling Lifetime length: 39.290
    Best Lifetime Rolling Avg: 52.857142857142854
    Best Episode: 6
    Time Since Best: 493
    Has Succeeded: False
    Stopped Improving: False
    Metrics: loss: 0.023562, mean_q: 0.120933, mean_eps: 0.956116
    Total Training Time: 106.792s
    
    -----------------
                    
    Episode: 750
    Step: 6816/1000000
    This Episode Steps: 17
    This Episode Reward: 0.0
    This Episode Duration: 0.458s
    Rolling Lifetime length: 40.450
    Best Lifetime Rolling Avg: 52.857142857142854
    Best Episode: 6
    Time Since Best: 743
    Has Succeeded: False
    Stopped Improving: False
    Metrics: loss: 0.036497, mean_q: 0.276420, mean_eps: 0.933291
    Total Training Time: 173.804s
    
    -----------------
                    
    Episode: 1000
    Step: 9114/1000000
    This Episode Steps: 8
    This Episode Reward: 1.0
    This Episode Duration: 0.242s
    Rolling Lifetime length: 39.430
    Best Lifetime Rolling Avg: 52.857142857142854
    Best Episode: 6
    Time Since Best: 993
    Has Succeeded: False
    Stopped Improving: False
    Metrics: loss: 0.025847, mean_q: 0.249734, mean_eps: 0.910727
    Total Training Time: 239.278s
    
    -----------------
                    
    Episode: 1250
    Step: 11476/1000000
    This Episode Steps: 8
    This Episode Reward: 3.0
    This Episode Duration: 0.244s
    Rolling Lifetime length: 41.060
    Best Lifetime Rolling Avg: 52.857142857142854
    Best Episode: 6
    Time Since Best: 1243
    Has Succeeded: False
    Stopped Improving: False
    Metrics: loss: 0.037298, mean_q: 0.447289, mean_eps: 0.887579
    Total Training Time: 307.324s
    
    -----------------
                    
    Episode: 1500
    Step: 13912/1000000
    This Episode Steps: 3
    This Episode Reward: 0.0
    This Episode Duration: 0.105s
    Rolling Lifetime length: 41.730
    Best Lifetime Rolling Avg: 52.857142857142854
    Best Episode: 6
    Time Since Best: 1493
    Has Succeeded: False
    Stopped Improving: False
    Metrics: loss: 0.039221, mean_q: 0.421553, mean_eps: 0.863682
    Total Training Time: 376.612s
    
    -----------------
                    
    Episode: 1750
    Step: 16078/1000000
    This Episode Steps: 3
    This Episode Reward: 0.0
    This Episode Duration: 0.102s
    Rolling Lifetime length: 40.190
    Best Lifetime Rolling Avg: 52.857142857142854
    Best Episode: 6
    Time Since Best: 1743
    Has Succeeded: False
    Stopped Improving: False
    Metrics: loss: 0.064389, mean_q: 0.455956, mean_eps: 0.842455
    Total Training Time: 438.599s
    
    -----------------
                    
    Episode: 2000
    Step: 18717/1000000
    This Episode Steps: 8
    This Episode Reward: 1.0
    This Episode Duration: 0.230s
    Rolling Lifetime length: 42.200
    Best Lifetime Rolling Avg: 52.857142857142854
    Best Episode: 6
    Time Since Best: 1993
    Has Succeeded: False
    Stopped Improving: False
    Metrics: loss: 0.036992, mean_q: 0.554969, mean_eps: 0.816617
    Total Training Time: 512.786s
    
    -----------------
                    
    Episode: 2250
    Step: 21381/1000000
    This Episode Steps: 14
    This Episode Reward: 0.0
    This Episode Duration: 0.390s
    Rolling Lifetime length: 45.030
    Best Lifetime Rolling Avg: 52.857142857142854
    Best Episode: 6
    Time Since Best: 2243
    Has Succeeded: False
    Stopped Improving: False
    Metrics: loss: 0.045886, mean_q: 0.642659, mean_eps: 0.790540
    Total Training Time: 587.940s
    
    -----------------
                    
    Episode: 2500
    Step: 23909/1000000
    This Episode Steps: 5
    This Episode Reward: 0.0
    This Episode Duration: 0.160s
    Rolling Lifetime length: 46.660
    Best Lifetime Rolling Avg: 52.857142857142854
    Best Episode: 6
    Time Since Best: 2493
    Has Succeeded: False
    Stopped Improving: False
    Metrics: loss: 0.054479, mean_q: 0.717090, mean_eps: 0.765721
    Total Training Time: 660.193s
    
    -----------------
                    
    Episode: 2750
    Step: 26466/1000000
    This Episode Steps: 10
    This Episode Reward: 0.0
    This Episode Duration: 0.299s
    Rolling Lifetime length: 45.160
    Best Lifetime Rolling Avg: 52.857142857142854
    Best Episode: 6
    Time Since Best: 2743
    Has Succeeded: False
    Stopped Improving: False
    Metrics: loss: 0.051640, mean_q: 0.803924, mean_eps: 0.740687
    Total Training Time: 733.650s
    
    -----------------
                    
    Episode: 3000
    Step: 29116/1000000
    This Episode Steps: 10
    This Episode Reward: 0.0
    This Episode Duration: 0.286s
    Rolling Lifetime length: 45.770
    Best Lifetime Rolling Avg: 52.857142857142854
    Best Episode: 6
    Time Since Best: 2993
    Has Succeeded: False
    Stopped Improving: False
    Metrics: loss: 0.069156, mean_q: 0.757156, mean_eps: 0.714717
    Total Training Time: 809.657s
    
    -----------------
                    
    Episode: 3250
    Step: 32038/1000000
    This Episode Steps: 7
    This Episode Reward: 1.0
    This Episode Duration: 0.210s
    Rolling Lifetime length: 49.800
    Best Lifetime Rolling Avg: 52.857142857142854
    Best Episode: 6
    Time Since Best: 3243
    Has Succeeded: False
    Stopped Improving: False
    Metrics: loss: 0.060753, mean_q: 0.862240, mean_eps: 0.686067
    Total Training Time: 892.617s
    
    -----------------
                    
    Episode: 3500
    Step: 35087/1000000
    This Episode Steps: 4
    This Episode Reward: 0.0
    This Episode Duration: 0.131s
    Rolling Lifetime length: 51.020
    Best Lifetime Rolling Avg: 52.857142857142854
    Best Episode: 6
    Time Since Best: 3493
    Has Succeeded: False
    Stopped Improving: False
    Metrics: loss: 0.104373, mean_q: 1.079528, mean_eps: 0.656172
    Total Training Time: 978.630s
    
    -----------------
                    
    Episode: 3750
    Step: 38207/1000000
    This Episode Steps: 9
    This Episode Reward: 0.0
    This Episode Duration: 0.256s
    Rolling Lifetime length: 53.970
    Best Lifetime Rolling Avg: 54.44
    Best Episode: 3696
    Time Since Best: 53
    Has Succeeded: False
    Stopped Improving: False
    Metrics: loss: 0.080510, mean_q: 1.064533, mean_eps: 0.625620
    Total Training Time: 1066.612s
    
    -----------------
                    
    Episode: 4000
    Step: 41734/1000000
    This Episode Steps: 8
    This Episode Reward: 0.0
    This Episode Duration: 0.222s
    Rolling Lifetime length: 60.340
    Best Lifetime Rolling Avg: 60.48
    Best Episode: 3993
    Time Since Best: 6
    Has Succeeded: False
    Stopped Improving: False
    Metrics: loss: 0.057967, mean_q: 1.162211, mean_eps: 0.591051
    Total Training Time: 1165.103s
    
    -----------------
                    
    Episode: 4250
    Step: 44888/1000000
    This Episode Steps: 30
    This Episode Reward: 1.0
    This Episode Duration: 0.789s
    Rolling Lifetime length: 61.910
    Best Lifetime Rolling Avg: 62.12
    Best Episode: 4080
    Time Since Best: 169
    Has Succeeded: False
    Stopped Improving: False
    Metrics: loss: 0.078224, mean_q: 1.276537, mean_eps: 0.560249
    Total Training Time: 1254.294s
    
    -----------------
                    
    Episode: 4500
    Step: 48271/1000000
    This Episode Steps: 5
    This Episode Reward: 1.0
    This Episode Duration: 0.159s
    Rolling Lifetime length: 62.600
    Best Lifetime Rolling Avg: 62.78
    Best Episode: 4439
    Time Since Best: 60
    Has Succeeded: False
    Stopped Improving: False
    Metrics: loss: 0.091546, mean_q: 1.308860, mean_eps: 0.526974
    Total Training Time: 1349.351s
    
    -----------------
                    
    Episode: 4750
    Step: 52236/1000000
    This Episode Steps: 27
    This Episode Reward: 1.0
    This Episode Duration: 0.729s
    Rolling Lifetime length: 68.760
    Best Lifetime Rolling Avg: 68.78
    Best Episode: 4747
    Time Since Best: 2
    Has Succeeded: False
    Stopped Improving: False
    Metrics: loss: 0.142958, mean_q: 1.590916, mean_eps: 0.488224
    Total Training Time: 1459.410s
    
    -----------------
                    
    Episode: 5000
    Step: 56625/1000000
    This Episode Steps: 17
    This Episode Reward: 3.0
    This Episode Duration: 0.474s
    Rolling Lifetime length: 75.710
    Best Lifetime Rolling Avg: 75.71
    Best Episode: 4999
    Time Since Best: 0
    Has Succeeded: False
    Stopped Improving: False
    Metrics: loss: 0.162122, mean_q: 1.784167, mean_eps: 0.445163
    Total Training Time: 1581.474s
    
    -----------------
                    
    Episode: 5250
    Step: 61647/1000000
    This Episode Steps: 72
    This Episode Reward: 27.0
    This Episode Duration: 1.902s
    Rolling Lifetime length: 83.940
    Best Lifetime Rolling Avg: 83.94
    Best Episode: 5249
    Time Since Best: 0
    Has Succeeded: False
    Stopped Improving: False
    Metrics: loss: 0.168767, mean_q: 2.040635, mean_eps: 0.396217
    Total Training Time: 1719.928s
    
    -----------------
                    
    Episode: 5500
    Step: 67704/1000000
    This Episode Steps: 9
    This Episode Reward: 2.0
    This Episode Duration: 0.262s
    Rolling Lifetime length: 106.920
    Best Lifetime Rolling Avg: 107.24
    Best Episode: 5483
    Time Since Best: 16
    Has Succeeded: False
    Stopped Improving: False
    Metrics: loss: 0.224453, mean_q: 2.257581, mean_eps: 0.336550
    Total Training Time: 1885.324s
    
    -----------------
                    
    Episode: 5750
    Step: 75097/1000000
    This Episode Steps: 115
    This Episode Reward: 50.0
    This Episode Duration: 3.092s
    Rolling Lifetime length: 143.220
    Best Lifetime Rolling Avg: 143.22
    Best Episode: 5749
    Time Since Best: 0
    Has Succeeded: False
    Stopped Improving: False
    Metrics: loss: 0.279388, mean_q: 3.023634, mean_eps: 0.264618
    Total Training Time: 2086.488s
    
    -----------------
                    
    Episode: 6000
    Step: 89611/1000000
    This Episode Steps: 37
    This Episode Reward: 24.0
    This Episode Duration: 1.061s
    Rolling Lifetime length: 279.420
    Best Lifetime Rolling Avg: 279.42
    Best Episode: 5999
    Time Since Best: 0
    Has Succeeded: False
    Stopped Improving: False
    Metrics: loss: 0.343747, mean_q: 4.159633, mean_eps: 0.121998
    Total Training Time: 2478.817s
    
    -----------------
                    
    Episode: 6250
    Step: 126262/1000000
    This Episode Steps: 784
    This Episode Reward: 566.0
    This Episode Duration: 21.441s
    Rolling Lifetime length: 1020.830
    Best Lifetime Rolling Avg: 1020.83
    Best Episode: 6249
    Time Since Best: 0
    Has Succeeded: False
    Stopped Improving: False
    Metrics: loss: 0.580168, mean_q: 9.287714, mean_eps: 0.020000
    Total Training Time: 3490.853s
    
    Training Finished in 5840.354 seconds
            
    Final Step: 210321
    Succeeded: False
    Stopped_Improving: False
    Final Episode Lifetimes Rolling Avg: 2882.750



```python
weights_file = os.path.join(logging_directory, "dqn_weights.h5f")
dqn.save_weights(weights_file, overwrite=True)
```


```python
from matplotlib import pyplot as plt
%matplotlib inline

training_history = history.history["episode_lifetimes_rolling_avg"]

plt.figure(figsize=(12,7))
plt.plot(training_history)
plt.xlabel('Episode')
plt.ylabel('Rolling Average Qubit Lifetime')
_ = plt.title("Training History")
```


![png](output_8_0.png)


