{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) Evaluating and Running Decoders with Stim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we begin by importing the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import gym\n",
    "import keras\n",
    "import numpy as np3\n",
    "import rl as rl\n",
    "\n",
    "# import shutilL\n",
    "import tensorflow\n",
    "from Environments import *\n",
    "from Function_Library import *\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.callbacks import FileLogger\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import (\n",
    "    BoltzmannQPolicy,\n",
    "    EpsGreedyQPolicy,\n",
    "    GreedyQPolicy,\n",
    "    LinearAnnealedPolicy,\n",
    ")\n",
    "from Utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to load:\n",
    "    \n",
    "   1. The hyper-parameters of the agent we would like to test\n",
    "   2. The weights of the agent\n",
    "    \n",
    "In this example we will evaluate one of the provided pre-trained decoders, for d=5, with X noise only, trained at an error rate of p_phys=p_meas=0.007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_configs_path = os.path.join(os.getcwd(), \"../trained_models/d5_x/fixed_config.p\")\n",
    "variable_configs_path = os.path.join(\n",
    "    os.getcwd(), \"../trained_models/d5_x/0.007/variable_config_77.p\"\n",
    ")\n",
    "model_weights_path = os.path.join(\n",
    "    os.getcwd(), \"../trained_models/d5_x/0.007/final_dqn_weights.h5f\"\n",
    ")\n",
    "\n",
    "static_decoder_path = os.path.join(os.getcwd(), \"referee_decoders/nn_d5_X_p5\")\n",
    "static_decoder = load_model(static_decoder_path)\n",
    "\n",
    "fixed_configs = pickle.load(open(fixed_configs_path, \"rb\"))\n",
    "variable_configs = pickle.load(open(variable_configs_path, \"rb\"))\n",
    "\n",
    "all_configs = {}\n",
    "\n",
    "for key in fixed_configs.keys():\n",
    "    all_configs[key] = fixed_configs[key]\n",
    "\n",
    "for key in variable_configs.keys():\n",
    "    all_configs[key] = variable_configs[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Surface_Code_Environment_Multi_Decoding_Cycles(\n",
    "    d=all_configs[\"d\"],\n",
    "    p_phys=all_configs[\"p_phys\"],\n",
    "    p_meas=all_configs[\"p_meas\"],\n",
    "    error_model=all_configs[\"error_model\"],\n",
    "    use_Y=all_configs[\"use_Y\"],\n",
    "    volume_depth=all_configs[\"volume_depth\"],\n",
    "    static_decoder=static_decoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build a model and instantiate an agent with all the parameters of the pre-trained agent. Notice that we insist on a greedy policy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_convolutional_nn(\n",
    "    all_configs[\"c_layers\"],\n",
    "    all_configs[\"ff_layers\"],\n",
    "    env.observation_space.shape,\n",
    "    env.num_actions,\n",
    ")\n",
    "memory = SequentialMemory(limit=all_configs[\"buffer_size\"], window_length=1)\n",
    "policy = GreedyQPolicy(masked_greedy=True)\n",
    "test_policy = GreedyQPolicy(masked_greedy=True)\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "dqn = DQNAgent(\n",
    "    model=model,\n",
    "    nb_actions=env.num_actions,\n",
    "    memory=memory,\n",
    "    nb_steps_warmup=all_configs[\"learning_starts\"],\n",
    "    target_model_update=all_configs[\"target_network_update_freq\"],\n",
    "    policy=policy,\n",
    "    test_policy=test_policy,\n",
    "    gamma=all_configs[\"gamma\"],\n",
    "    enable_dueling_network=all_configs[\"dueling\"],\n",
    ")\n",
    "\n",
    "\n",
    "dqn.compile(Adam(lr=all_configs[\"learning_rate\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage the agent has random weights, and so we load in the weights of the pre-trained agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.model.load_weights(model_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generating syndromes for the rotated surface code with stim\n",
    "\n",
    "We would like to demonstrate how to use the decoder with stim. This allows us to simulate circuit level noise.\n",
    "\n",
    "To do this, we start by generating syndromes from a surface code using stim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = 5\n",
    "batch_size = 10000\n",
    "p_phys = 0.008\n",
    "# p_meas=p_phys\n",
    "\n",
    "# circuit = stim.Circuit.generated(\n",
    "#     \"surface_code:rotated_memory_z\",\n",
    "#     rounds=d+1,\n",
    "#     distance=5,\n",
    "#     # before_round_data_depolarization=p_phys)\n",
    "#     before_measure_flip_probability=p_meas)\n",
    "# print(repr(circuit))\n",
    "\n",
    "# check if qubit flips are applied in each syndrome measurement round!\n",
    "circuit_file = open(\"sf-d5-memory.stim\")\n",
    "circuit_file = circuit_file.read().replace(\"0.007\", str(p_phys))\n",
    "circuit = stim.Circuit(circuit_file)\n",
    "# print(circuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for sampling from detectors (which introduces temporal dependencies)\n",
    "detector_sampler = circuit.compile_detector_sampler()\n",
    "shots = detector_sampler.sample(batch_size, append_observables=True)\n",
    "\n",
    "# first cycle: (d**2-1)/2 detector measurements.\n",
    "# second cycle:, (d**2-1)/2 X/Z and Z/X checks\n",
    "detector_parts = shots[:, : circuit.num_detectors]\n",
    "# logical observable along boundary\n",
    "actual_observable_parts = shots[:, circuit.num_detectors :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mround = d**2 - 1\n",
    "\n",
    "# for t in range(batch_size):\n",
    "#     print()\n",
    "#     for s in range(d+2):\n",
    "#         print(detector_parts[t][s * mround : (s + 1) * mround])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the syndromes and observable, we extract the syndrome coordinates and transform the syndromes to a numpy array that gets accepted by the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "mround = d**2 - 1\n",
    "syndrome_coords = circuit.get_detector_coordinates()\n",
    "syndrome_coords = (\n",
    "    np.array([syndrome_coords[e][0:2] for e in range(len(syndrome_coords))], int) // 2\n",
    ")\n",
    "\n",
    "def get_syndrome(syndromes, coords):\n",
    "    faulty_syndrome = np.zeros((d + 1, d + 1), int)\n",
    "    for e in range(len(syndromes)):\n",
    "        i, j = coords[e]\n",
    "        faulty_syndrome[i, j] = syndromes[e]\n",
    "    # We need to flip the surface code around vertical axis\n",
    "    # faulty_syndrome = np.fliplr(faulty_syndrome)\n",
    "    faulty_syndrome = np.flipud(faulty_syndrome)\n",
    "    return faulty_syndrome\n",
    "\n",
    "\n",
    "faulty_syndromes = np.zeros((batch_size, d, d + 1, d + 1), int)\n",
    "for t in range(batch_size):\n",
    "    for s in range(d):\n",
    "        faulty_syndromes[t][s] = get_syndrome(\n",
    "            detector_parts[t][(s+1) * mround : (s + 2) * mround],\n",
    "            syndrome_coords[(s+1) * mround : (s + 2) * mround],\n",
    "        )\n",
    "\n",
    "# detectors detect differences between time steps.\n",
    "# We want absolute values for each time step.\n",
    "for s in range(1, d):\n",
    "    faulty_syndromes[:, s] += faulty_syndromes[:, s - 1]\n",
    "    faulty_syndromes[:, s] = faulty_syndromes[:, s] % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(faulty_syndromes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to decode and obtain the suggested corrections. To do this, we begin by padding the faulty syndromes as required and by concatenating the obtained volume with an action history slice, in which all the actions are initially zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize a zero'd input volume\n",
    "input_state = np.zeros((batch_size, d + 1, 2 * d + 1, 2 * d + 1), int)\n",
    "\n",
    "# embed and place the faulty syndrome slices in the correct place\n",
    "for t in range(batch_size):\n",
    "    for j in range(d):\n",
    "        # TODO: padding_syndrome is hardcoded for d=5??\n",
    "        input_state[t, j, :, :] = env.padding_syndrome(faulty_syndromes[t][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can run the agent, collecting the suggested actions, until the agent does the identity, which suggests that it is finished decoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections = []\n",
    "\n",
    "for t in range(batch_size):\n",
    "    corrections.append([])\n",
    "    still_decoding = True\n",
    "    while still_decoding:\n",
    "\n",
    "        # Fetch the suggested correction\n",
    "        action = dqn.forward(input_state[t])\n",
    "\n",
    "        if action not in corrections[t] and action != env.identity_index:\n",
    "            # If the action has not yet been done, or is not the identity\n",
    "\n",
    "            # append the suggested correction to the list of corrections\n",
    "            corrections[t].append(action)\n",
    "\n",
    "            # Update the input state to the agent to indicate the correction it would have made\n",
    "            input_state[t, d, :, :] = env.padding_actions(corrections[t])\n",
    "\n",
    "        else:\n",
    "            # decoding should stop\n",
    "            still_decoding = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can view the suggested corrections, which in this case was a single correct suggestion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicate which qubits have been flipped\n",
    "corrected_state = np.zeros((batch_size, d, d), int)\n",
    "corrected_syndromes = np.zeros((batch_size, d+1, d+1), int)\n",
    "corrected_logical_obs = np.zeros(batch_size, int)\n",
    "for t in range(batch_size):\n",
    "    # print(\"-------------\")\n",
    "    # get measured logical qubit\n",
    "    corrected_logical_obs[t] = actual_observable_parts[t][0]\n",
    "    for correction in corrections[t]:\n",
    "        \n",
    "        # update data qubit state\n",
    "        col = correction % d\n",
    "        row = (correction - col) // d\n",
    "        corrected_state[t, row, col] = 1\n",
    "        # print(f\"Correcting qubit {correction} at ({row+1},{col+1})\")\n",
    "        \n",
    "        # update syndrome\n",
    "        for k in range(4):\n",
    "            if env.qubits[row,col,k,2] == 3:\n",
    "                (x,y) = env.qubits[row,col,k,:2]\n",
    "                corrected_syndromes[t,x,y]=1\n",
    "                \n",
    "        # update logical observable\n",
    "        if col == 0:\n",
    "            corrected_logical_obs[t] = (corrected_logical_obs[t] + 1) % 2\n",
    "    # print(f\"Observable flipped: {actual_observable_parts[t][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some pretty drawings to verify the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly predicted: 0.8565\n"
     ]
    }
   ],
   "source": [
    "reward = 0\n",
    "for t in range(batch_size):\n",
    "    final_syndrome = get_syndrome(\n",
    "        detector_parts[t][(d+1) * mround : -1], syndrome_coords[(d+1) * mround : -1]\n",
    "    )\n",
    "    # prepare updated syndrome for referee decoder\n",
    "    corrected_syndromes[t] = (corrected_syndromes[t] + final_syndrome) % 2\n",
    "    true_syndrome_vector = np.reshape(corrected_syndromes[t],(env.d+1)**2) \n",
    "    # referee\n",
    "    referee_label = env.static_decoder.predict(np.array([true_syndrome_vector]), batch_size=1, verbose=0)\n",
    "    reward += (np.argmax(referee_label) == actual_observable_parts[t][0])\n",
    "    # print(f\"Predicted: {bool(np.argmax(referee_label))}, actual: {actual_observable_parts[t][0]}\")\n",
    "    \n",
    "    # draw_surface_code(corrected_state[t], env.syndromes, final_syndrome, env.d)\n",
    "    # draw faulty syndromes to understand deepq's decision\n",
    "    # for s in range(d):\n",
    "    #     draw_surface_code(env.hidden_state, env.syndromes, faulty_syndromes[t][s], env.d)\n",
    "print(f\"Correctly predicted: {reward/batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in general if there is more than one error, or if the agent is uncertain about a given configuration, it may choose to do the identity, therefore triggering a new syndrome volume from which it may be more certain which action to take - The crucial point is that in practice we are interested in how long the qubit survives for, and an optimal strategy for achieving long qubit lifetimes may not be to attempt to fully decode into the ground state after each syndrome volume!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepq",
   "language": "python",
   "name": "deepq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
