{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Evaluating and Running Decoders\n",
    "\n",
    "Now that we know how to train a decoder, we would like to see how to evaluate the performance of that decoder, as well as how to use the decoder in a production setting. In this notebook we will demonstrate how to perform both of these tasks.\n",
    "\n",
    "##### 3a) Evaluating a Trained Decoder\n",
    "\n",
    "Given a trained decoder we would of course like to benchmark the decoder to evaluate how well it performs. This procedure is very similar to training the decoder, in that we run multiple decoding episodes in which the agent interacts with the environment until it \"dies\" - however in this context we would like the agent to use only a greedy policy for action selection, i.e. to never make random moves, and we do not need to update the agents parameters in time. As we will see benchmarking an agent is made easy by use of the DQNAgent class \"test\" method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we begin by importing the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow\n",
    "import gym\n",
    "\n",
    "from Function_Library import *\n",
    "from Environments import *\n",
    "\n",
    "import rl as rl\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy, LinearAnnealedPolicy, GreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.callbacks import FileLogger\n",
    "\n",
    "import json\n",
    "import copy\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to load:\n",
    "    \n",
    "   1. The hyper-parameters of the agent we would like to test\n",
    "   2. The weights of the agent\n",
    "    \n",
    "In this example we will evaluate one of the provided pre-trained decoders, for d=5, with X noise only, trained at an error rate of p_phys=p_meas=0.007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_configs_path = os.path.join(os.getcwd(),\"../trained_models/d5_x/fixed_config.p\")\n",
    "variable_configs_path = os.path.join(os.getcwd(),\"../trained_models/d5_x/0.007/variable_config_77.p\")\n",
    "model_weights_path = os.path.join(os.getcwd(),\"../trained_models/d5_x/0.007/final_dqn_weights.h5f\")\n",
    "\n",
    "static_decoder_path = os.path.join(os.getcwd(),\"referee_decoders/nn_d5_X_p5\")\n",
    "static_decoder = load_model(static_decoder_path)\n",
    "\n",
    "fixed_configs = pickle.load( open(fixed_configs_path, \"rb\" ) )\n",
    "variable_configs = pickle.load( open(variable_configs_path, \"rb\" ) )\n",
    "\n",
    "all_configs = {}\n",
    "\n",
    "for key in fixed_configs.keys():\n",
    "    all_configs[key] = fixed_configs[key]\n",
    "\n",
    "for key in variable_configs.keys():\n",
    "    all_configs[key] = variable_configs[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can instantiate the environment in which we will test the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Surface_Code_Environment_Multi_Decoding_Cycles(d=all_configs[\"d\"], \n",
    "    p_phys=all_configs[\"p_phys\"], \n",
    "    p_meas=all_configs[\"p_meas\"],  \n",
    "    error_model=all_configs[\"error_model\"], \n",
    "    use_Y=all_configs[\"use_Y\"], \n",
    "    volume_depth=all_configs[\"volume_depth\"],\n",
    "    static_decoder=static_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build a model and instantiate an agent with all the parameters of the pre-trained agent. Notice that we insist on a greedy policy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_convolutional_nn(all_configs[\"c_layers\"],all_configs[\"ff_layers\"], \n",
    "                               env.observation_space.shape, env.num_actions)\n",
    "memory = SequentialMemory(limit=all_configs[\"buffer_size\"], window_length=1)\n",
    "policy = GreedyQPolicy(masked_greedy=True)\n",
    "test_policy = GreedyQPolicy(masked_greedy=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "dqn = DQNAgent(model=model, \n",
    "               nb_actions=env.num_actions, \n",
    "               memory=memory, \n",
    "               nb_steps_warmup=all_configs[\"learning_starts\"], \n",
    "               target_model_update=all_configs[\"target_network_update_freq\"], \n",
    "               policy=policy,\n",
    "               test_policy=test_policy,\n",
    "               gamma = all_configs[\"gamma\"],\n",
    "               enable_dueling_network=all_configs[\"dueling\"])  \n",
    "\n",
    "\n",
    "dqn.compile(Adam(lr=all_configs[\"learning_rate\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage the agent has random weights, and so we load in the weights of the pre-trained agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.model.load_weights(model_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now finally we can benchmark the agent using the test method. \n",
    "\n",
    "It is important to note that the reported episode length is the number of _non-trivial_ syndrome volumes that the agent received, as these are the steps during which a decision needs to be taken on the part of the agent. The qubit lifetime, whose rolling average is reported, is the total number of syndrome measurements (between which an error may occur) for which the agent survived, as this is the relevant metric to compare with a single faulty qubit whose expected lifetime is 1/(error_probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 1001 episodes ...\n",
      "-----------------\n",
      "Episode: 1\n",
      "This Episode Length: 44\n",
      "This Episode Reward: 27.0\n",
      "This Episode Lifetime: 125\n",
      "\n",
      "Episode Lifetimes Avg: 125.000\n",
      "\n",
      "-----------------\n",
      "Episode: 101\n",
      "This Episode Length: 278\n",
      "This Episode Reward: 171.0\n",
      "This Episode Lifetime: 830\n",
      "\n",
      "Episode Lifetimes Avg: 330.149\n",
      "\n",
      "-----------------\n",
      "Episode: 201\n",
      "This Episode Length: 94\n",
      "This Episode Reward: 42.0\n",
      "This Episode Lifetime: 225\n",
      "\n",
      "Episode Lifetimes Avg: 371.667\n",
      "\n",
      "-----------------\n",
      "Episode: 301\n",
      "This Episode Length: 96\n",
      "This Episode Reward: 47.0\n",
      "This Episode Lifetime: 240\n",
      "\n",
      "Episode Lifetimes Avg: 349.568\n",
      "\n",
      "-----------------\n",
      "Episode: 401\n",
      "This Episode Length: 43\n",
      "This Episode Reward: 21.0\n",
      "This Episode Lifetime: 135\n",
      "\n",
      "Episode Lifetimes Avg: 354.514\n",
      "\n",
      "-----------------\n",
      "Episode: 501\n",
      "This Episode Length: 98\n",
      "This Episode Reward: 67.0\n",
      "This Episode Lifetime: 280\n",
      "\n",
      "Episode Lifetimes Avg: 357.016\n",
      "\n",
      "-----------------\n",
      "Episode: 601\n",
      "This Episode Length: 102\n",
      "This Episode Reward: 78.0\n",
      "This Episode Lifetime: 340\n",
      "\n",
      "Episode Lifetimes Avg: 347.870\n",
      "\n",
      "-----------------\n",
      "Episode: 701\n",
      "This Episode Length: 187\n",
      "This Episode Reward: 119.0\n",
      "This Episode Lifetime: 575\n",
      "\n",
      "Episode Lifetimes Avg: 339.137\n",
      "\n",
      "-----------------\n",
      "Episode: 801\n",
      "This Episode Length: 18\n",
      "This Episode Reward: 11.0\n",
      "This Episode Lifetime: 45\n",
      "\n",
      "Episode Lifetimes Avg: 336.941\n",
      "\n",
      "-----------------\n",
      "Episode: 901\n",
      "This Episode Length: 230\n",
      "This Episode Reward: 172.0\n",
      "This Episode Lifetime: 685\n",
      "\n",
      "Episode Lifetimes Avg: 330.638\n",
      "\n",
      "-----------------\n",
      "Episode: 1001\n",
      "This Episode Length: 280\n",
      "This Episode Reward: 166.0\n",
      "This Episode Lifetime: 765\n",
      "\n",
      "Episode Lifetimes Avg: 329.106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_test_episodes = 1001\n",
    "testing_history = dqn.test(env,nb_episodes = nb_test_episodes, visualize=False, verbose=2, \n",
    "                           interval=100, single_cycle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Qubit Lifetime: 329.1058941058941\n"
     ]
    }
   ],
   "source": [
    "results = testing_history.history[\"episode_lifetime\"]\n",
    "\n",
    "print(\"Mean Qubit Lifetime:\", np.mean(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that on average, over 1001 test episodes, the qubit survives for 329 syndrome measurements on average, which is better than the average lifetime of 143 syndrome measurements for a single faulty qubit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3b) Using a Trained Decoder in Production\n",
    "\n",
    "In addition to benchmarking a decoder via the agent test method, we would like to demonstrate how to use the decoder in practice, given a faulty syndrome volume. In principle all the information on how to do this is contained within the environments and test method, but to aid in applying these decoders quickly and easily in practice we make everything explicit here:\n",
    "\n",
    "To do this, we start by generating a faulty syndrome volume as would be generated by an experiment or in the process of a quantum computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=5\n",
    "p_phys=0.007\n",
    "p_meas=p_phys\n",
    "error_model = \"X\"\n",
    "qubits = generateSurfaceCodeLattice(d)\n",
    "\n",
    "hidden_state = np.zeros((d, d), int)\n",
    "\n",
    "faulty_syndromes = []\n",
    "for j in range(d):\n",
    "    error = generate_error(d, p_phys, error_model)\n",
    "    hidden_state = obtain_new_error_configuration(hidden_state, error)\n",
    "    current_true_syndrome = generate_surface_code_syndrome_NoFT_efficient(hidden_state, qubits)\n",
    "    current_faulty_syndrome = generate_faulty_syndrome(current_true_syndrome, p_meas)\n",
    "    faulty_syndromes.append(current_faulty_syndrome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By viewing the final hidden_state (the lattice state) we can see what errors occured, which here was a single error on the 21st qubit (we start counting from 0, and move row wise left to right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can view the faulty_syndromes that we received, which is what would come out of an experiment. As we can see, measurement errors occured in syndrome slices 2 and 5, and it appears as if the actual error occured between extraction of syndrome 2 and 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syndrome slice 1\n",
      "\n",
      "[[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]]\n",
      "\n",
      "syndrome slice 2\n",
      "\n",
      "[[0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]]\n",
      "\n",
      "syndrome slice 3\n",
      "\n",
      "[[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0]]\n",
      "\n",
      "syndrome slice 4\n",
      "\n",
      "[[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0]]\n",
      "\n",
      "syndrome slice 5\n",
      "\n",
      "[[0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for j in range(d):\n",
    "    print(\"syndrome slice\", j+1)\n",
    "    print()\n",
    "    print(faulty_syndromes[j])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we would like to decode and obtain the suggested corrections. To do this, we begin by padding the faulty syndromes as required and by concatenating the obtained volume with an action history slice, in which all the actions are initially zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize a zero'd input volume\n",
    "input_state = np.zeros((d+1, 2*d + 1, 2*d + 1),int)\n",
    "\n",
    "# embed and place the faulty syndrome slices in the correct place\n",
    "for j in range(d):\n",
    "            input_state[j, :, :] = env.padding_syndrome(faulty_syndromes[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can run the agent, collecting the suggested actions, until the agent does the identity, which suggests that it is finished decoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections = []\n",
    "\n",
    "still_decoding = True\n",
    "while still_decoding:\n",
    "    \n",
    "    # Fetch the suggested correction\n",
    "    action = dqn.forward(input_state)\n",
    "    \n",
    "    if action not in corrections and action != env.identity_index:\n",
    "        # If the action has not yet been done, or is not the identity\n",
    "        \n",
    "        # append the suggested correction to the list of corrections\n",
    "        corrections.append(action)\n",
    "        \n",
    "        # Update the input state to the agent to indicate the correction it would have made\n",
    "        input_state[d, :, :] = env.padding_actions(corrections)\n",
    "        \n",
    "    else:\n",
    "        # decoding should stop\n",
    "        still_decoding = False\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can view the suggested corrections, which in this case was a single correct suggestion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21]\n"
     ]
    }
   ],
   "source": [
    "print(corrections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in general if there is more than one error, or if the agent is uncertain about a given configuration, it may choose to do the identity, therefore triggering a new syndrome volume from which it may be more certain which action to take - The crucial point is that in practice we are interested in how long the qubit survives for, and an optimal strategy for achieving long qubit lifetimes may not be to attempt to fully decode into the ground state after each syndrome volume!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
