\documentclass[twocolumn,preprintnumbers,amsmath,amssymb,notitlepage,nofootinbib,longbibliography,superscriptaddress,aps,pra,10pt]{revtex4-1}
%\documentclass[preprint,showpacs,preprintnumbers,amsmath,amssymb]{revtex4}

% Packages and custom commands
	\usepackage{natbib}
	\usepackage{graphicx}
	\usepackage{dcolumn}
	\usepackage{dsfont}
	\usepackage{bm}
	%\usepackage{float}

	\usepackage[usenames, dvipsnames]{color}
	\usepackage{url}
	\usepackage[colorlinks=true,breaklinks=true,allcolors=blue]{hyperref}
	\usepackage{microtype}
	\DeclareMathOperator{\dist}{\text{dist}}

	\newtheorem{theorem}{Theorem}
	\newtheorem{lemma}{Lemma}

	\newcommand{\ryan}[1]{\textcolor{Cyan}{RS: #1}}

	\newcommand{\mk}[1]{\textcolor{Cyan}{MK: #1}}

	\newcommand{\en}[1]{\textcolor{Cyan}{EN: #1}}

	\newcommand{\je}[1]{\textcolor{Cyan}{JE: #1}}

\begin{document}

% Title page
	\title{DeepQ Decoding for Fault Tolerant Quantum Computation}

	\author{Ryan Sweke}
	\affiliation{\mbox{Dahlem Center for Complex Quantum Systems, Freie Universit\"{a}t Berlin, 14195 Berlin, Germany}}
	\author{Markus S. Kesselring}
	\affiliation{\mbox{Dahlem Center for Complex Quantum Systems, Freie Universit\"{a}t Berlin, 14195 Berlin, Germany}}
	\author{Evert P.L. van Nieuwenburg}
	\affiliation{\mbox{Institute for Quantum Information and Matter, Caltech, Pasadena, CA 91125, USA}}
	\author{Jens Eisert}
	\affiliation{\mbox{Dahlem Center for Complex Quantum Systems, Freie Universit\"{a}t Berlin, 14195 Berlin, Germany}}


	\date{\today}

% Abstract
	\begin{abstract}
		Topological error correcting codes, and particularly the surface code, currently provide the most feasible roadmap towards large-scale fault tolerant quantum computation.
		As such, obtaining fast and flexible decoding algorithms for these codes, within the experimentally relevant context of faulty syndrome measurements, is of critical importance.
		In this work we show that the problem of decoding such codes, in the full fault tolerant setting, can be naturally reformulated as a process of repeated interactions between a decoding agent and a code environment, to which the machinery of reinforcement learning can be applied to obtain decoding agents.
		As a demonstration, by using deepQ learning, we obtain fast decoding agents for the surface code, for a variety of noise-models, within the fully fault tolerant setting.
	\end{abstract}

\maketitle

\section{Introduction}\label{s:introduction}

	In order to implement large scale quantum algorithms it is necessary to be able to store and manipulate quantum information in a manner
	that is robust to the unavoidable errors introduced through interaction of the physical qubits with a noisy environment.
	A useful strategy for achieving such robustness is to encode a single logical qubit into the state of many physical qubits,
	via a quantum error correcting code, from which it may be possible to actively diagnose and correct errors that may
	occur~\cite{Terhal15,Campbell17}. While many quantum error correcting codes exist, topological quantum
	codes~\cite{Kitaev03, Dennis02, Preskill17lectures, Nayak08, Pachos12, Terhal15, Brown16, Campbell17}, in which only local operations
	are required to diagnose and correct errors, are of particular interest as a result of their
	experimental feasibility~\cite{Reed12, Barends14, Nigg14, Corcoles15, Albrecht16, Takita16, Linke17}.
	Out of the topological quantum codes, the recently emerged surface code is an especially promising candidate for large-scale
	fault-tolerant quantum computation due to the combination of its comparatively low overhead and locality requirements, coupled
	with the availability of convenient strategies for the implementation of all required logical gates.

	In any code-based strategy for fault tolerant quantum computation, decoding algorithms play a critical role.
	At a high level these algorithms take as input the outcomes of diagnostic syndrome measurements (which characterize the
	error that occured at the lower level of the physical qubits), and provide as output a suggestion of
	corrections for any errors that might have occured during the computation. These corrections can then be applied either immediately or at
	another desired moment to obtain the correct results.

	It is particularly important to note that in any physically realistic setting the required syndrome measurements are obtained
	via small quantum circuit measurements too, and are therefore generically also faulty themselves.
	As such, while the setting of perfect syndrome measurements provides a good test-bed for the development of decoding algorithms, any
	decoding algorithm which aims to be experimentally useful should also be capable of dealing with such faulty syndrome measurements.
	Additionally, such algorithms should also be capable of dealing with experimentally relevant noise models, as well as be fast
	enough to not present a bottleneck to the execution of computations, even as the size of the system (i.e. the code distance) grows.

	Due to the importance of decoding algorithms for fault tolerant quantum computation, many different approaches have been developed
	and investigated, each of which tries to satisfy as many of the experimentally required criteria as possible.
	Perhaps most prominent are algorithms based on minimum-weight perfect matching subroutines~\cite{Fowler13},
	although alternative approaches based on techniques such as the renormalization group~\cite{Duclos2010} and
	locally operating cellular automata~\cite{Herold15, Kubica2018} have also been put forward as promising candidates.

	Recently, techniques from machine learning have begun to find application in diverse areas of quantum physics - such as in the efficient
	representation of many-body quantum states~\cite{Carleo16,todo}, the identification of phase transitions~\cite{Melko, Liu, todo}, and
	the autonomous design of novel experimental set-ups~\cite{Bruegel, Coles, todo} - and various neural-network based decoders have
	also been proposed~\cite{Torlai10, Varsamopoulos17, Krastanov17, Breuckmann18, Baireuther18a, Baireuther18b, Ni18}.
	Despite this diversity of decoding algorithms, there is as of yet no algorithm which clearly satisfies all
	the required experimental criteria listed above, nor a clear consensus as to which technique would be the most experimentally feasible.
	\mk{
		I think this is a bit to harsh towards MWPM/blossom, which I would say still is the best candidate and probably will one day be used for really large codes.
		I think we should rather say: for near term realisations of quantum error correction codes, until MWPM gets faster, a flexible and fast decoder is needed.
		We put forward RL decoding as one candidate.
		(This also does not put us in too tight a spot on delivering ideas how to tackle larger code distances.)
	}
	In particular, while the previously proposed neural network decoders promise extremely fast decoding times and
	flexibility with respect to the underlying noise model~\cite{Ronagh16}, all such decoders
	are restricted either to the setting of perfect syndrome measurements or to the setting of preserving a logical qubit for as long as possible.
	In the latter scenario there is no requirement on being able to perform a subsequent logical gate, and hence there is room for
	improvement regarding both scenarios.

	Simultaneously, the last few years have also seen impressive advances in the development of deep reinforcement learning algorithms,
	which have allowed for the training of neural network based agents capable of obtaining super-human performance in domains such
	as Atari~\cite{Mnih15}, Chess~\cite{Silver17a} and Go~\cite{Silver17b}.
	These techniques are particularly powerful in situations where it is necessary to learn strategies for complex sequential decision
	making, involving consideration of the future effects of ones actions. In this work we demonstrate that decoding in the fault-tolerant
	setting provides a natural setting for the application of reinforcement learning methods. We will discuss the advantages of such
	an approach over the previously proposed network decoders.

	In particular, we reformulate the problem of decoding as an agent performing discrete, sequential actions on the code (which is
	referred to as the environment).
	This reframing of the code as a reinforcement learning environment immediately allows for various different learning agents
	to attempt to learn to decode. In our current approach, we will show that a deepQ-learning agent can decode the surface code for
	a variety of noise models in the fully fault-tolerant setting.
	These results then provide a foundation for extension via both more sophisticated reinforcement learning techniques and
	more sophisticated neural network models.

	In this work we begin by providing an introductory overview of the surface code in Section~\ref{s:the_surface_code},
	before presenting a description of the decoding problem for fault-tolerant quantum computation in Section~\ref{s:the_decoding_problem}.
	After a brief introduction to the formalism of reinforcement learning and $Q$-functions in Section~\ref{s:reinforcement_learning} we
	are then able to provide the conceptual reframing of decoding as a reinforcement learning problem in Section~\ref{s:decoding_as_rl}, representing
	one of the primary results of this work.
	In Section~\ref{s:results} we then present deepQ surface code decoders, before finally in Section~\ref{s:conclusions} we
	discuss both the advantages and disadvantages of the approach presented here,
	along with various potential strategies for building upon the results presented in this work.

\section{The Surface Code}\label{s:the_surface_code}

	We begin by providing a brief description of the surface code.
	The framework and methods presented in this work are not restricted to the surface code however, and may be applied to any stabilizer code.
	The restriction to the surface code is made both for simplicity of presentation and experimental relevance.
	We will focus on presenting the essential elements of the surface code necessary for understanding the decoding problem, and refer to
	more complete treatments for details~\cite{a,b,c}.

	We will consider $d\times d$ lattices with a \textit{physical} data qubit on each vertex $v$, as illustrated in Fig.~\ref{f:surface_code} for $d=5$.
	The collective state of all qubits on the lattice is an element of the Hilbert space $\mathcal{H} = \mathbb{C}^{2^{(d\times d)}}$.
	We associate stabilizer operators with each colored plaquette of the lattice, with blue (orange) plaquettes representing operators
	applying Pauli $X$ ($Z$) flips to the qubits at the vertices of the plaquettes.
	Specifically, denoting the set of all blue (orange) plaquettes as $B_p$ ($O_p$) we define the stabilizer $S_p$ on plaquette $p$ as,

	\begin{equation}\label{e:stabilizer_definition}
		S_p = \bigotimes_{v\in p} \sigma_v \quad \text{where }
		\begin{cases}
			\sigma_v = X_v \quad \text{if } p \in B_p,\\
			\sigma_v = Z_v \quad \text{if } p \in O_p.
		\end{cases}
	\end{equation}
	All stabilizers are mutually commuting and have eigenvalues $\pm 1$. This allows for the introduction
	of a Hamiltonian $H = -\sum_p S_p$, from which the surface code $\mathcal{H}_\mathrm{sc} \subset \mathcal{H}$ is now defined as
	the space of ground states of $H$. This space consists of all states for which $S_p = +1$ for all stabilizers, and for the surface
	code is two dimensional (i.e. $\mathcal{H}_\mathrm{sc} \simeq \mathbb{C}^2$) and can hence encode a single \textit{logical} qubit.
	Logical operators are operators which preserve the code space by keeping the state in $\mathcal{H}_\mathrm{sc}$ and thereby
	manipulate the state of the logical qubit.
	Figure~\ref{f:surface_code} shows logical $X$ ($Z$) operators, denoted $X_L$ ($Z_L$), which are continuous strings of single
	vertex $X$ ($Z$) operators which connect the top and bottom (left and right) boundaries of the lattice.

	\begin{figure}
		\centering
		\includegraphics[width=0.8\linewidth]{figures/surface_code.pdf}
		\caption{
			An overview of the $5 \times 5$ surface code.
			(a) We consider square lattices, with a data qubit on each vertex of the lattice.
			The coloured plaquettes indicate stabilizer operators as defined in Eq.~\eqref{e:stabilizer_definition}.
			(b) Logical $X_L$ and $Z_L$ operators for the surface code are given by continuous strings of single qubit $X$ or $Z$ operators connecting the top and bottom or left and right boundaries of the code respectively.
		}
		\label{f:surface_code}
	\end{figure}

	To illustrate the motivation behind such an encoding, let's examine the consequences of a single qubit Pauli flip on a physical data qubit.
	If we assume that the initial state $|\psi\rangle \in \mathcal{H}_\mathrm{sc}$ is an element of the code space, then the resulting state
	after having incurred a single qubit flip, $|\psi'\rangle \not \in \mathcal{H}_\mathrm{sc}$, will no longer be an element of the code space.
	In particular, $|\psi'\rangle$ will be an eigenstate with eigenvalue $-1$ of at least one stabilizer.
	We say that $|\psi'\rangle$ violates these stabilizers, as illustrated by red circles in Fig.~\ref{f:surface_code_examples}~(a).
	The \textit{syndrome} of a state is the string encoding the outcomes of a simultaneous measurement of all the stabilizers,
	each of which takes the value $\pm 1$. The syndrome hence characterizes the physical errors that occured. If only a single error occured,
	we may be able to correct it and preserve the logical qubit's state. The problem of decoding in the case of multiple errors is
	much harder, as will be described in the next section.

	\begin{figure}
		\centering
		\includegraphics[width=1\linewidth]{figures/surface_code_examples.pdf}
		\caption{
			(a) Single qubit Pauli flips violate surrounding stabilizers.
			(b-d) Strings of Pauli flips only violate stabilizers at the endpoint of the string.
			Multiple error configurations can give rise to the same syndrome.
			They can differ by stabilizers, as for example in (c) and (d), or by logical operators, see (b) and (c).
		}
		\label{f:surface_code_examples}
	\end{figure}

\section{The Decoding Problem}\label{s:the_decoding_problem}

	With the foundations from the previous section, we can now formulate a simple preliminary version of the decoding problem: \newline

	\noindent\textit{Assume that at} $t=0$ \textit{one is given a state} $|\psi\rangle = \alpha |0_L\rangle + \beta |1_L\rangle \in \mathcal{H}_{\mathrm{sc}}.$
	\textit{At some time }$t_1>0$ \textit{a syndrome measurement is performed which indicates that one or more stabilizers are violated - i.e. some errors have occurred on physical data qubits.
	From the given syndrome, determine a set of corrections which should be applied to the code lattice such that the subsequent state} $|\psi'\rangle$
	\textit{is equal to the initial state} $|\psi\rangle.$ \newline

	\begin{figure*}
		\centering
		\includegraphics[width=0.6\textwidth]{figures/decoding_problem.pdf}
		\caption{
			A typical decoding cycle is illustrated for the simplified faulty measurements scenario in which one imagines each time step consisting of an initial physical
			error process generating errors on the data qubits, followed by a second measurement error process which corrupts the true syndrome.
			The decoding algorithm then has access to a sequence of potentially faulty syndromes.
		}
		\label{f:decoding_problem}
	\end{figure*}

	\noindent Before proceeding to discuss more subtle and technical versions of the decoding problem, let's examine why even the above problem is indeed potentially difficult.
	The most important observation is that the map from error configurations to syndromes is many-to-one, i.e. many different sets of errors can lead to the same syndrome.
	As an example, consider the error configurations illustrated in lattices (b), (c) and (d) of Fig.~\ref{f:surface_code_examples}, all of which lead to the same syndrome.
	If the probability of an error on a single physical data qubit is low, given such a syndrome one might reasonably assume that the error in lattice (c) occurred,
	as one error on a single qubit is a more likely event than errors on multiple qubits.
	Given this reasoning, one might then suggest to correct by applying an $X$ flip on the physical data qubit in the third row and fourth column.
	If indeed the error in lattice (c) occurred the resulting state would be error-free and the initial state is preserved.
	However, if in reality the error pattern shown in (d) occurred, this set of errors combined with the applied $X$ flip would implement a stabilizer.
	Since the original state was a simultaneous $+1$ eigenstate of all the stabilizers and stabilizers act trivially on logical states, the
	proposed correction indeed preserves the initial logical state.
	Finally, if the error in lattice (b) occurred, then the combination of the original error with the correction would implement the logical $X_L$ operator.
	This operator keeps the state in the code space $\mathcal{H}_\textrm{sc}$, but it will be in a different logical state. Hence the information we were trying to
	preserve has been altered. From this simple example one can see that most often solving the decoding problem as stated above involves deciding,
	given an inherently ambiguous syndrome and (possibly imperfect) knowledge of the underlying error model, which error configuration most likely occurred.

	To complicate matters more, in addition to the inherent difficulty resulting from syndrome ambiguity the process of extracting the syndrome is also subject to noise,
	and hence the syndrome \textit{itself} may be faulty. The measurement of the syndrome is done, in practice, through the use of ancilla qubits. Each stabilizer operator
	is associated to an ancilla qubit, which is entangled with each of the physical data qubits corresponding to the stabilizer. This entangling operation is done through
	the execution of a small quantum circuit, after which a measurement of the ancilla qubit provides the $\pm 1$ result of the stabilizer.
	In order to fully account for errors during the process of syndrome extraction, it is therefore important to model this entire circuit in which errors can occur on
	both the data qubits and ancilla qubits at each time step. Moreoever, errors on the ancilla qubits can propagate onto the data qubits via the required entangling gates.

	The essential aspect of the additional difficulty from faulty syndrome measurements can be phenomenologically modelled by imagining each time step as consisting
	of two distinct error processes, as illustrated in Fig.~\ref{f:decoding_problem}. In the first error process, an error occurs on each data qubit with some probability.
	One then imagines extracting the perfect syndrome before a second error process occurs, in which with a given probability an error occurs on each stabilizer measurement outcome.
	In this scenario, single syndrome measurements are hence no longer reliable. Decoding in the fault tolerant setting therefore typically requires providing a set of (sequential) syndrome
	measurements.

	Finally, in the context of surface code based fault tolerant quantum computing, all logical gates are implemented either via protocols which also involve an inherent
	decoding procedure or do not spread errors.
	To be specific, it is sufficient for universal quantum computing to be able to implement both Clifford and $T$ gates~\cite{todo}.
	In contemporary proposals for surface code based quantum computing protocols are known for implementing Clifford gates either transversally, via code deformation or via lattice surgery.
	While transversal gates do not spread errors, code deformation and lattice surgery require decoding cycles.
	Non-Clifford gates, such as the $T$ gate, can be performed fault tolerantly via gate teleportation using magic states.
	High quality magic states can be obtained via magic state distillation, which requires only Clifford gates and initially faulty magic states.
	As such the goal of decoding idling logical qubits within a quantum computation should be to suppress errors to the extent that any of the above procedures can succeed with high probability.
	Therefore, we can relax the requirement that the decoding process should return the post-error state to the initial state in the code space.
	In Section~\ref{s:decoding_as_rl} we discuss a proxy criterion for decoding success within this framework.

\section{Reinforcement Learning and q-Functions}\label{s:reinforcement_learning}

	In this section we shift focus and introduce some of the fundamental concepts of reinforcement learning and $q$-functions, which will be essential to our rephrasing
	of the decoding problem in Section \ref{s:decoding_as_rl}. In this section too, we will constrain ourselves to the basics and refer to Ref.~\cite{BartoSutton} for a complete treatment.

	Reinforcement learning considers an agent interacting with an environment, as is illustrated in Fig.~\ref{f:agent_environment}.
	The agent can act on and observe (parts of) the environment, and is tasked with achieving a problem-specific goal by performing a sequence of actions.
	We typically consider discrete problems, in which at each step $t$ we describe the environment as a state $S_t$. The agent can choose to perform an action $A_t$ taken from
	the set of possible actions $\{A\}$ (referred to as the action space), after which the environment updates accordingly and the agent recieves feedback on its choice in the form of a scalar reward denoted $R_t$.

	We will specify to episodic environments, in which a terminal state exists (e.g. ``game over'') after which the environment is reset.
	The agent's choice of action, the resulting state of the environment and the returned reward can all be stochastic. In the situation of finite state
	and action spaces, this process can be formalized within the framework of finite Markov decision processes (FMDPs):

	\begin{equation}
		p(s',r|s,a) \equiv \mathrm{pr}(S_t = s',R_t = r|S_{t-1} = s, A_{t-1} = a).
	\end{equation}
	To formalize the decision making process of the agent, we define an agent's strategy (its \textit{policy}) $\pi$, as a mapping from states to probabilities
	of specific actions - i.e. $\pi(a|s)$ is the probability that the agent chooses $A_t = a$ given that the environment is in state $S_t = s$.
	For FMDP's we then define the \textit{value} of a state $s$ under policy $\pi$ as,

	\begin{equation}
		v_{\pi}(s) = \mathbb{E}_{\pi}[G_t|S_t = s]  = \mathbb{E}_{\pi} \Big[\sum_{k = 0}^{\infty}\gamma^k R_{t+k+1}\Big| S_t = s \Big]
	\end{equation}
	$\forall S_t \in \mathcal{S}$, where $S$ is the space of all possible states of the environment. The term $G_t$ is the discounted return (discounted cumulative reward),
	with discount factor $0 \leq \gamma \leq 1$, and is the quantity that the agent is tasked with optimizing. In practice, the infinite sum terminates whenever state $S_{t+k+1}$ is a terminal state.
	We call $v$ the \textit{state-value function}, providing the expected discounted cumulative reward the agent would obtain when following policy $\pi$ from state $s$.

	It is an important conceptual point to note that by using the metric of the discounted cumulative reward, the value of any given state depends not only on the
	immediate reward of a chosen action, but includes previous and future expected rewards. Hence strategies which involve some element of successful future planning
	may lead to higher expected state values.
	As such, we see that the value of a state with respect to a given policy reflects accurately the ability of an agent to achieve its long-term goals when
	following that policy from that state.

	Similarly to the \textit{state-value function}, we can define the \textit{action-value function} (referred to as the $q$-function) for policy $\pi$ via:

	\begin{align}
		q_{\pi}(s,a) &= \mathbb{E}_{\pi}[G_t|S_t = s, A_t = a]  \\
		& = \mathbb{E}_{\pi} \Big[\sum_{k = 0}^{\infty}\gamma^k R_{t+k+1}\Big| S_t = s, A_t = a \Big]
	\end{align}
	Clearly, the $q$-function with respect to a given policy is conceptually similar to the value function with the exception that in addition we specify
	the choice of action. Importantly, value functions allow us to sort policies, i.e. $\pi > \pi' \iff v_{\pi}(s) > v_{\pi'}(s)\quad \forall s \in \mathcal{S} $.
	That in turn allows us to specify an optimal policy $\pi^*$ for which:

	\begin{figure}
		\centering
		\includegraphics[width=0.4\textwidth]{figures/agent_environment.pdf}
		\caption{
			An illustration of the signals passed between an agent and the environment through the duration of a sequential turn based episode.
		}
		\label{f:agent_environment}
	\end{figure}

	\begin{equation}\label{e:fixed_point}
		q_*(s,a) = \mathbb{E}\big[R_{t+1} + \gamma\max_{a'}q_{*}(S_{t+1},a')\big|S_t = s, A_t = a \big].
	\end{equation}
	Note that given the optimal $q$-function it is easy to obtain the optimal strategy; In a given state $s$ simply choose the action $a = \mathrm{argmax}_{a'}[q_*(s,a')]$.

	Our task is then to find a procedure by which we can find or approximate $q_*(s,a)$. This is generically done via iterative $q$-learning, in which the agent
	starts with an arbitrary initial $q$-function and iteratively refines it by playing episodes of the environment. Hence the agent's $q$-function is used to
	choose actions to interact with the environment (possibly including other explorative methods of choosing actions),
	and is updated using Eq.~\eqref{e:fixed_point} for which $q_*(s,a)$ is a stationary solution.

	The above describes concisely the elements of reinforcement learning, but did not address the general impracticality of storing the $q$-function. In most real-world applications,
	the number of states for the environment can be impractically large (e.g. consider the number of possible chess configurations). This is where \textit{deep-Q} learning
	makes a difference. Namely, in deepQ learning we parametrize $q$ by a neural network, and use Eq.~\eqref{e:fixed_point} to construct the cost function from which the
	network weights can be updated via a stochastic gradient descent method. The learning of the $q$-function is hence done by training a neural network in a supervised manner,
	the samples for which are generated by having the agent explore the environment. In particular, we let the agent interact with the environment via an
	$\epsilon$-greedy exploration policy, in the process generating experience-tuples of the following form:

	\begin{equation}
		[S_t,A_t,R_{t+1},S_{t+1}]
	\end{equation}
	These tuples provide the data-set we use to train the $q$-network via the cost function:

	\begin{align}
		C &= y_{\mathrm{pred}} - y_{\mathrm{true}}\\
		&= q(S_t,A_t) - \big[R_{t+1} + \gamma\max_{a'}q(S_{t+1},a') \big]
	\end{align}
	which, by comparison with Eq.~\eqref{e:fixed_point}, will be minimized only for the optimal policy.
	Unfortunately, despite the simplicity of this idea, in practice a variety of tricks - such as batched experience replay, separate active and target $q$-networks,
	double-$q$ learning and dueling networks - are required to achieve stable $q$-learning.	We refer to the relevant references, or to the associated code repository, for details.

	\en{Maybe a small paragraph on agents in general here? Something like: It is important to notice that the choice of our agent, a deepQ network (DQN), is not the only
	possible choice. Current state-of-the-art agents such as those mentioned in the previous paragraph often outperform DQNs in specific tasks. Conceptually however, DQNs are the
	simplest form of neural network based agents, and we have found that they perform well on the decoding task that we will describe next.}

\section{Decoding as a Reinforcement Learning Problem}\label{s:decoding_as_rl}

	We now turn to describing the main topic of this work. Namely, we formulate the \textit{fault tolerant} decoding problem for idling logical qubits as a reinforcement learning problem.
	In particular, as we will see in Section~\ref{s:results}, the advantage of this reframing is that a reinforcement learned agent learns to solve the decoding problem without prior
	knowledge of the underlying error model. We will next describe a single episode of the decoding environment, illustrated in Fig.~\ref{f:single_episode}.

	\begin{figure*}
		\centering
		\includegraphics[width=0.6\textwidth]{figures/decoding_as_rl.pdf}
		\caption{
			An illustration of the various steps occurring within a single episode.
			(a) In an initial step, a faulty syndrome volume is extracted from a state initially in the code space.
			This faulty syndrome volume is combined with an initially empty action history and passed to the agent as the initial state.
			Given an input state, the agent must decide on an action, which it passes to the environment.
			If the action is neither an identity or a repetition of an action already in the action history (b), then the procedure illustrated in box (c) occurs, if a trivial or already performed action is chosen (d), then the procedure illustrated in box (e) occurs.
			(c) The chosen action is applied to the underlying quantum state.
			From this state the reward for the applied action is determined, while simultaneously a referee decoder decides whether or not the episode is now over - i.e.
			whether the underlying quantum state is now in a terminal state.
			Additionally, the applied action is appended to the action history, which is concatenated with the non-updated syndrome volume and provided to the agent, in conjunction with the reward and terminal state indicator.
			(e) Once again, the underlying quantum state is first updated, and from this updated state both a reward and terminal state indicator are determined (not shown).
			However, the trivial or repeated action of the agent triggers a new round of faulty syndrome measurements.
			After this new round of syndrome extraction, the new syndrome volume is combined with a reset action history and provided to the agent, from which it can once again choose another move.
		}
		\label{f:single_episode}
	\end{figure*}

	Our environment is the surface code, and the actions for the agent consist of either performing a Pauli $X$ or $Z$ bit-flip on a chosen data qubit or otherwise do nothing (i.e. perform
	an identity operation). A single episode starts with a new syndrome measurement,
	and ends whenever the agent is unable to correctly decode it. To be more specific, each episode is played according to the following steps:

	Figure~\ref{f:single_episode} (a) shows the first step ($t=0$) of any episode. The environment is reset to a new initial state $S_1$, and the reward $R_1$ and an extra
	boolean indicator $T_1$ representing whether the environment is in a terminal (e.g. ``game over'') state as follows.
	\begin{enumerate}
		\item First the state of the surface code, which we will refer to as the \textit{hidden state} of the environment, is initialized to some logical state
		$|\psi_0\rangle \in \mathcal{H_\mathrm{sc}}$. The agent's task will be to preserve this state. Since this state is in the code space by construction, it can be simply
		represented as an empty list representing a code lattice on which no Pauli flips have been applied to any physical data qubits.

		\item Then, a new random sequence of faulty syndromes is generated. We utilize the two-stage error process involving separated physical and measurement errors
		as described in Fig.~\ref{s:the_decoding_problem}, with specific error rates. We wish to emphasise however that in principle a circuit model for syndrome extraction could also be simulated.
		The errors on physical data qubits which were generated during the syndrome extraction protocol are then added to the hidden state of the environment, since we will need them
		later to evaluate the agent's choice of actions. The agent has no knowledge of this hidden state.

		\item The visible state, which the agent can observe, is then constructed from the sequence of faulty syndromes, illustrated by the green slices in Fig.~\ref{f:single_episode}, which we refer to
		as the \textit {syndrome volume} $S_{E,1}$. In addition to the syndrome volume, the total state received by the agent at any time step also contains a list of the actions taken by the agent since
		the last new syndrome volume was received, which we illustrate in Fig.~\ref{f:single_episode} with the yellow slices and refer to as the action history $h$.
		Since for this first step a new syndrome volume was just extracted, the action history of the agent is reset - i.e. $h_1$ is an empty list - and this reset action history is
		combined with the syndrome volume and provided to the agent as the total output of the environment - i.e. $S_1 = (S_{E,1},h_1)$.

		\item The agent has not yet taken any actions, so the reward for this first step is $R_1 =0$. Since we generated a state with errors by construction, the terminal state indicator is
		set to false - i.e. $T_1=0$.
	\end{enumerate}

	%Note that at any stage during an episode this hidden state is a list containing all errors and corrections applied to the physical data qubits throughout the course of the episode.
	%We can think of this hidden state list as representing the underlying error-plus-corrections configuration of the underlying code lattice, however at any point the current state of
	%the system $|\psi\rangle$ could be obtained by applying all operations listed in the hidden state of the environment to the initial state $|\psi_0\rangle$.
	%In later steps of the episode this hidden state will be used to determine rewards and terminal states, however the agent will never have direct access to this state.

	After this sequence of initialization, the agent needs to choose an action $A_t$ to apply to the environment, after which the environment needs
	to respond with the tuple $(S_{t+1}, R_{t+1}, T_{t+1})$, where $S_{t+1}=(S_{E,{t+1}},h_{t+1})$. We assign a special interpretation to the identity action of the agent,
	using it as a signal of the agent to request a new syndrome volume. In other words, the agent should learn that whenever it believes it has decoded the problem, it can
	signal so by performing an identity action in situations where no more syndrome remains.

	Depending on the action chosen by the agent, the environment can respond in one of two ways, illustrated in Fig.~\ref{f:single_episode} (c) and (e) respectively.

	\begin{figure*}
		\centering
		\includegraphics[width=0.6\textwidth]{figures/agent_decoding.pdf}
		\caption{
			The procedure for decoding with a trained agent.
			(a) Given a faulty syndrome volume generated by some experiment, this volume is concatenated with an empty action history to produce a suitable input state for the decoding agent.
			(b) The decoding agent takes in the combined faulty syndrome volume and action history and chooses an action.
			This action is simultaneously (c) added to a list of accumulated corrections and (d) used to update only the action history component of the input state to the agent.
			This updated input state is then given to the agent and procedures (b-d) continue until the agent (e) repeats an action or chooses the identity.
			(f) At this stage the corrections could be applied to the lattice state, although in practice they would be tracked through the entire computation.
		}
		\label{agent_decoding}
	\end{figure*}

  \subsection{Bit-flip and non-repeated actions}
	If, as shown in Fig.~\ref{f:single_episode} (c), the agent chooses any non-identity action that it is not already in the action history list
	then this step of the episode continues via the following process;

	\begin{enumerate}
		\item Firstly, given the chosen non-trivial action $A_t$, the first step is to apply this action to the code lattice - i.e. the agents chosen action is added to the hidden
		state of the environment, which is now a combination of all actions and errors on physical data qubits.
		\item The environment now needs to determine the reward $R_{t+1}$ the agent should receive for its action and whether the resutling hidden state is a terminal state.
		As was discussed in Section~\ref{f:decoding_problem}, within the context of surface code based fault tolerant quantum computation, the goal of the decoding process for idling
		logical qubits should be to suppress errors to the extent that future logical operations (which either involve decoding cycles of their own or do not spread errors) can succeed
		with high probability. As such, if after application of the agent's chosen action the current state of the system $|\psi\rangle$ is equal to the original state $|\psi_0\rangle$ -
		or equivalently, if all errors have been corrected without implementing a logical operation - we give the agent a reward of $+1$, i.e. $R_{t+1} = +1$.
		In all other cases we set $R_{t+1} = 0$, giving the agent no reward.
		\item In addition to the above, we introduce a separate \textit{referee} decoder to act as a proxy for future logical operations. This referee decoder should be a
		``single-shot" decoder, which, given a single perfect syndrome, suggests corrections which always move the current state back into the code space. The referee may fail by
		inadvertently performing a logical operation however. After the hidden state of the environment is updated to include the agents chosen action, this referee decoder is then
		given a perfect syndrome extracted from the state $|\psi\rangle$ corresponding to the current hidden state. If the referee decoder is able to correctly decode this perfect syndrome
		we are not in a terminal state and set $T_{t+1} = 0$. If on the other hand not even the referee can decode the perfect syndrome we set $T_{t+1}=1$, which indicates the end of the episode.
		\item Finally, the new environment's state $S_{t+1} = (S_{E,t+1},h_{t+1}) = (S_{E,t},h_{t+1})$ is constructed from the updated action history $h_{t+1}$ (consisting of $h_t$ appended by
		$A_t$) and the $S_{E,t+1} = S_{E,t}$ since we did not extract a new syndrome volume.
	\end{enumerate}

	\begin{figure*}
		\centering
		\includegraphics[width=0.9\textwidth]{figures/agent.pdf}
		\caption{
			Details of a deepQ decoding agent for a $d\times d$ code lattice.
			(a) A single faulty syndrome is embedded into a $(2d +1)\times(2d+1)$ binary matrix, where by default all entries are initially set to $0$.
			Entries indicate by orange (blue) circles are then used to indicate violated $Z$ ($X$) stabilizers on the corresponding plaquette.
			Entries indicated by black circles are set to $+1$ as a way of differentiating different types of stabilizers.
			(b) Using the same embedding, the action history for $Z$ ($X$) flips can be encoded by indicating a previous flip on a specific vertex qubit via the entry corresponding to that vertex.
			(c) By stacking the syndrome and action history slices, the total state $S_t$ can be constructed in a form suitable for input to a convolutional neural network.
			(d) To complete the deepQ network, we stack a feed forward neural network on top of the convolutional layers.
			In particular, the final layer of this network has $|\mathcal{A}|$ activations, each of which encodes $q(S_t,a)$ for an action $a$.
		}
		\label{f:agent}
	\end{figure*}

	\subsection{Identity or repeated actions}
	If on the other hand the agent chooses either the identity action or an action it has already chosen (which has the effect of undoing this previously chosen correction),
	then the environment responds via the following procedure, as illustrated in Fig.~\ref{f:single_episode} (e).

	If the action is a repeated action, the first step is to apply the action to the code lattice by appending the action to the hidden state.
	In this case, the reward $R_{t+1}$ and terminal state indicator $T_{t+1}$ are determined by the procedure already described above for non-repeated actions.
	If the chosen action was the identity however, then this hidden state update is not necessary, and given that the previous hidden state could not have been a terminal state we set $T_{t+1} = 0$,
	before setting $R_{t+1} = R_t$ since the state hasn't changed.
	Now, we interpret the performing of an identity (either explicitly via the identity action or implicitly by repeating a correction twice) by the agent as a request for a new syndrome,
	and hence a new syndrome volume is generated.
	Similarly to the initial step of the episode, the hidden state is then updated to include the errors on physical data qubits introduced via this syndrome extraction.
	Additionally, the action history $h_{t+1}$ is once again reset to an empty list, while the visible state $S_{E,t+1}$ is updated to reflect the newly extracted syndrome volume.
	Finally, the total output state $S_{t+1} = (S_{E,t+1},h_{t+1})$ is returned to the agent.


	In Section~\ref{s:dq_agent} we will present a detailed construction for a deepQ agent that allows us to apply deepQ-learning and obtain decoding agents whose
	performance is shown in Section~\ref{s:results}. Before proceeding however, it is worthwhile to emphasize a few points.

	First, it is important to note that the above agent-interaction framework is both code- and error-model agnostic,
	provided one has access to a referee decoder capable of single-shot decoding on perfect syndromes.
	From this perspective one might view the framework presented here as a tool for leveraging single-shot, perfect syndrome decoding algorithms,
	into fully fault-tolerant decoding algorithms.
	Additionally, it should be clear from the rules via which rewards are assigned, that the agent will only accumulate reward provided it can learn to consistently correct
	any errors that might occur on physical data qubits, without implementing logical operations.
	However, the agent is \textit{not} constrained to return the state back into the code space between successive syndrome measurements.
	In particular, because typical reinforcement learning algorithms take into account not only immediate rewards, but rather discounted cumulative rewards, the agent may learn
	strategies involving actions whose benefit may not be immediate.
	For example, if a rare event occurs and multiple measurement errors occur within the extraction of a syndrome volume, creating a highly ambiguous and difficult to decode input state,
	the agent may choose to only partially decode before requesting a new syndrome volume, in which hopefully less measurement errors will occur and the underlying error configuration may be
	less ambiguous. The decoding agents obtained via this framework may have access to truly novel decoding strategies that are not currently available to alternative decoding algorithms.

	It is also useful to note that in practice it is possible to speed-up learning by utilizing various improvements.
	First of all, when choosing exploratory actions, we can restrict the action space to a reduced set of potentially valid corrections.
	In particular, the agent need only consider actions on vertices either involved in a violated stabilizer or adjacent to vertices which have already been acted on.
	This restriction effectively increases the probability of the agent discovering useful actions, and therefore the effectiveness of any exploration phase.
	Second, when generating new syndrome volumes, we discard those cases in which the syndrome is trivial. In these cases, the agent does not need to act and no useful
	experience triplet is generated. This allows any experience memory required by the learning algorithm to consist of only useful memories.

	Finally, although we have now thoroughly addressed a framework to which reinforcement learning algorithms could be applied to \textit{train} an agent, we have not yet explicitly discussed
	the implementation of our network based agent and how it may be used to decode within an experimental setting.
	As illustrated in Fig.~\ref{agent_decoding}, the latter problem is straightforwardly adressed.
	Given a faulty syndrome volume from an experiment, we initialize an empty action history list and combine this with the faulty syndrome volume as an initial input to the decoding agent.
	The trained agent can then be asked directly for an initial correction, which is added to a list of accumulated actions. Simultaneously, the action history element of \textit{only} the
	previous input state is updated. This updated input state, containing the original syndrome volume and the subsequently performed corrections,
	is then given to the agent again as input. This process is iterated until the agent either repeats a correction or chooses the identity, which should be taken as a signal to perform
	another round of syndrome measurements.

\section{A deepQ Decoding Agent}\label{s:dq_agent}

	As discussed in the previous section, many different reinforcement learning algorithms could now be applied within the framework presented here.
	However, in order to provide a concrete example and proof-of-principle, we will specialize to deepQ learning, which has been previously utilized to obtain agents capable of human-level control in domains such as Atari.
	As mentioned briefly in Section~\ref{s:reinforcement_learning}, a variety of now standard tricks are required to get deepQ learning to work in practice, and we will not present these details here, referring the reader to the relevant references or associated code repository.
	However, there are various details concerning the construction of the deepQ agent which may be useful for applying alternative deep reinforcement learning algorithms within this framework, and as such we will present these details in this section.

	In particular, in Section~\ref{s:decoding_as_rl} we described how at the beginning of any time step $t$ the agent is supplied with the state $S_{t}=(S_{E,{t}},h_{t})$, where $S_{E,{t}}$ needs to be constructed from a faulty syndrome volume, and $h_{t}$ is a representation of the agents action history.
	In deepQ learning, and in many other deep reinforcement learning algorithms, this state $S_t$ needs to be provided as the input to a deep neural network.
	For example, in the case of deepQ learning, the state $S_t$ is the input to the deepQ network parametrizing the $q$-function from which the agent is partially deriving its policy.
	As such, utilizing an appropriate encoding of $S_t$, which allows for the use of appropriate neural networks, is important.

	In Fig.~\ref{f:agent} we have illustrated the encoding of $S_t$ which was used to facilitate the use of deep convolutional neural networks to parametrize the $q$-function.
	In particular, as shown in Fig.~\ref{f:agent} (a) and (b), we can embed a $d\times d$ code lattice into a $(2d + 1)\times(2d+1)$ binary matrix, where each entry corresponds to either a plaquette, a vertex, or an edge of the lattice.
	As shown in Fig.~\ref{f:agent} (a), we can then use a single such binary matrix to encode each of the faulty syndromes, by using the entries corresponding to plaquettes to indicate violated stabilizers, and the remaining entries to differentiate both blue and orange, and bulk and boundary plaquettes.
	Similarly, as illustrated in  Fig.~\ref{f:agent} (b) we can use two such binary matrices to encode the action history, by using one matrix to indicate the physical data qubits on which $X$ flips have already been applied, and the other binary matrix to indicate the data qubits on which $Z$ flips have already been applied.
	As can be seen in Fig.~\ref{f:agent} (c), the total state $S_t$ can then be obtained by stacking the action history slices on top of the faulty syndrome slices, effectively creating a multi-channel image suitable as input for a convolutional neural network.
	In particular, the strength of this encoding is that any convolutional filter, such as the one indicated in Fig.~\ref{f:agent} (c), then isolates all relevant information, both violated stabilizers and applied actions, from some local patch of the lattice.
	Finally, the deepQ network we utilize is completed by stacking a feed forward neural network on top of multiple convolutional layers.
	In particular, the final layer of this network has $|\mathcal{A}|$ activations, each of which encodes $q(S_t,a)$ for an action $a$.

\section{Results}\label{s:results}

	As a proof-of-principle, we utilized deepQ learning within the framework presented in Section~\ref{s:decoding_as_rl}, to obtain deepQ decoding agents for both bit-flip and depolarizing noise, for a $d=5$ surface code lattice.
	All the code utilized to obtain these agents is supplied in the corresponding \textit{DeepQ-Decoding} code repository, and as such we will provide only an overview of the details here.
	In particular, for a single fixed set of hyper-parameters, we used the original deepQ algorithm with annealed $\epsilon$-greedy exploration, implemented via the keras-RL library and incorporating doubleQ updates and dueling networks.
	In addition we used a custom training procedure, described in Appendix~\ref{A:training}, for sequentially iterating through increasing error rates, while simultaneously performing a hyper-parameter grid search at each error-rate iteration.
	For both error models we utilized a convolutional deepQ network, as illustrated in Fig.~\ref{f:agent}, consisting of three convolutional layers, followed by a single feed-forward layer before the final output layer.
	Specifically, if we describe a single convolutional layer with a three-tuple $[n,w,s]$, where $n$ is the number of filters, $w$ is the filter width and $s$ is the stride, and a single feed forward layer via a two-tuple $[n,d]$, where $n$ is the number of neurons and $d$ is the output drop-out rate, then from input to output our deepQ networks had the structure,

	\begin{figure}
		\centering
		\includegraphics[width=0.8\linewidth]{figures/results.png}
		\caption{
			These are the results.
		}
		\label{f:results}
	\end{figure}

	\begin{equation}
		[[64,3,2],~[32,2,1],~[32,2,1],~[512,0.2],~[|\mathcal{A}|, 0]].
	\end{equation}
	All other additional hyper-parameters used to obtain each decoding agent, along with histories of each training procedure, are provided in Appendix~\ref{A:parameters}.

	We then considered both bit-flip and depolarizing noise.
	For both error models we considered the measurement of a single syndrome to consist of two separate error channels, as illustrated in Fig.~\ref{f:decoding_problem}.
	For bit-flip noise, in the first error channel - the physical error channel - a Pauli $X$ flip was applied to each physical data qubit with probability $p_{\mathrm{phys}}$.
	For depolarising noise, the physical error channel acted by applying to each physical data qubit, with probability $p_{\mathrm{phys}}$, either a Pauli $X$, $Y$ or $Z$ flip, with equal probability.
	For both noise models, after the physical error channel, the true syndrome was calculated, after which the second measurement error channel was applied, in which the value of each stabilizer measurement was flipped with probability $p_{\mathrm{meas}}$.
	For all simulations we set $p \equiv p_{\mathrm{phys}} = p_{\mathrm{meas}}$.
	Also, note that for bit-flip noise, as only $X$ corrections were necessary, we had $|\mathcal{A}| = d^2 + 1$ - i.e. the agent could perform either the identity or a single qubit $X$ flip on any individual physical data qubit.
	For depolarising noise, we restricted the agent to only $X$ and $Z$ flips (as $Y$ errors can be corrected via both an $X$ flip and a $Z$ flip), such that $|\mathcal{A}| = 2d^2 + 1$.

	To evaluate the performance of our trained decoders we used the procedure described in Fig.~\ref{agent_decoding}, where the agent selected actions via the final $q$-function in a purely greedy manner, and with identities or repeated actions triggering new syndrome volumes.
	Importantly, we used the referee decoder to check after every action of the agent whether or not a terminal state had been reached.
	The qubit survival time of a single episode was then reported as the number of individual syndromes seen by the agent (i.e. the number of times the two-fold error channel was applied) before a terminal state was reached.
	Using the optimally trained decoder for each error rate, the results in Fig.~\ref{f:results} were then obtained by averaging over $10^3$ decoding episodes (each of which contained at least 100 decoding cycles on average).
	As can be seen in Fig.~\ref{f:results}, the average lifetime of the logical qubit was then compared with the average lifetime of a single faulty qubit.
	For bit-flip (depolarising) noise we find that for approximately $p < 7\times 10^{-3}$  $(p < 5\times 10^{-3})$ the decoded logical qubit outlasts a single faulty qubit on average.

	\ryan{TO DO:} There needs to be a more informed rigorous discussion of the "threshold" - i.e why we don't bother to calculate it precisely, how it compares to known results, and why we believe the results can be made significantly better.
	i.e. we need to really motivate that this is only a proof of principal, and that we believe it can be scaled in many directions.
	There also needs to an informed discussion of why we think these agents are ``fast'' - i.e. on dedicated hardware (asics or TPU's) how fast can you actually implement such a small neural network?

\section{Conclusion}\label{s:conclusions}

% Acknowledgments
	\begin{acknowledgments}
		The authors gratefully acknowledge helpful and insightful discussions with Daniel Litinski, Nicolas Delfosse, Aleksander Kubica, Thomas Jochym-O'Connor,
		Paul Baireuther, and Hendrik Poulsen Nautrup.
		Additionally, the authors would like to thank J\"{o}rg Behrmann for incredible technical support, without which this work would not have been possible.
		RS\ acknowledges the financial support of the Alexander von Humboldt foundation.
		MSK\ is supported by the DFG (CRC183, project B02).
		EvN\ is supported by the Swiss National Science Foundation through grant P2EZP2-172185.
		JE\ is supported by DFG (CRC 183, EI 519/14-1, and EI 519/7-1), the ERC (TAQ), the Templeton Foundation, and the BMBF (Q.com).
	\end{acknowledgments}

\appendix

\section{Distributed Iterative Training with Simultaneous Hyper-Parameter Optimization}\label{A:training}

\section{Agent Hyperparameters and Learning Curves}\label{A:parameters}

\bibliography{dq}

\end{document}
